import os
import re
import io
import json
import time
import math
import hmac
import base64
import datetime as dt
import textwrap
from dataclasses import dataclass
from typing import List, Dict, Any, Optional, Tuple

import requests
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt

from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas
from reportlab.lib.units import inch
from reportlab.lib import colors
from reportlab.lib.utils import ImageReader
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont


# =========================================================
# Page Config (MUST be called once)
# =========================================================
APP_TITLE = "AuraInsight æŠ¥å‘Šç”Ÿæˆå™¨ï¼ˆTrade Area & Growth Diagnosticï¼‰"
st.set_page_config(page_title=APP_TITLE, layout="wide")

OUTPUT_DIR = "output"
ASSETS_DIR = "assets"
FONTS_DIR = os.path.join(ASSETS_DIR, "fonts")

BG_COVER = os.path.join(ASSETS_DIR, "bg_cover.png")
BG_CONTENT = os.path.join(ASSETS_DIR, "bg_content.png")

FONT_NOTO_REG = os.path.join(FONTS_DIR, "NotoSansSC-Regular.ttf")
FONT_NOTO_BOLD = os.path.join(FONTS_DIR, "NotoSansSC-Bold.ttf")
FONT_NOTO_VAR = os.path.join(FONTS_DIR, "NotoSansSC-VariableFont_wght.ttf")

FONT_ROBOTO_REG = os.path.join(FONTS_DIR, "Roboto-Regular.ttf")
FONT_ROBOTO_BOLD = os.path.join(FONTS_DIR, "Roboto-Bold.ttf")
FONT_ROBOTO_ITALIC = os.path.join(FONTS_DIR, "Roboto-Italic.ttf")
FONT_ROBOTO_VAR = os.path.join(FONTS_DIR, "Roboto-VariableFont_wdth,wght.ttf")
FONT_ROBOTO_VAR_ITALIC = os.path.join(FONTS_DIR, "Roboto-Italic-VariableFont_wdth,wght.ttf")

PAGE_W, PAGE_H = letter


# =========================================================
# Auth
# =========================================================
def _get_allowed_passwords() -> List[str]:
    pw_list = st.secrets.get("ADMIN_PASSWORDS", None)
    if pw_list and isinstance(pw_list, (list, tuple)) and len(pw_list) > 0:
        return [str(x) for x in pw_list]
    single = st.secrets.get("ADMIN_PASSWORD", "")
    return [str(single)] if single else []

def _secure_compare(a: str, b: str) -> bool:
    return hmac.compare_digest(a.encode("utf-8"), b.encode("utf-8"))

def require_login():
    if "auth_ok" not in st.session_state:
        st.session_state.auth_ok = False
    if "auth_tries" not in st.session_state:
        st.session_state.auth_tries = 0
    if "auth_locked_until" not in st.session_state:
        st.session_state.auth_locked_until = 0.0

    if st.session_state.auth_ok:
        return

    now = time.time()
    if now < st.session_state.auth_locked_until:
        wait_s = int(st.session_state.auth_locked_until - now)
        st.error(f"å°è¯•æ¬¡æ•°è¿‡å¤šï¼Œè¯· {wait_s}s åå†è¯•ã€‚")
        st.stop()

    allowed = _get_allowed_passwords()
    if not allowed:
        st.error("æœªé…ç½®ç®¡ç†å‘˜å¯†ç ï¼šè¯·åœ¨ .streamlit/secrets.toml è®¾ç½® ADMIN_PASSWORD æˆ– ADMIN_PASSWORDSã€‚")
        st.stop()

    st.title("AuraInsight ç™»å½•")
    st.caption("è¯·è¾“å…¥ç®¡ç†å‘˜è®¾ç½®çš„å¯†ç åè¿›å…¥ã€‚")
    pw = st.text_input("å¯†ç ", type="password")

    c1, c2 = st.columns([1, 1])
    with c1:
        if st.button("ç™»å½•", type="primary"):
            ok = any(_secure_compare(pw, x) for x in allowed)
            if ok:
                st.session_state.auth_ok = True
                st.session_state.auth_tries = 0
                st.success("ç™»å½•æˆåŠŸã€‚")
                st.rerun()
            else:
                st.session_state.auth_tries += 1
                st.error("å¯†ç é”™è¯¯ã€‚")
                if st.session_state.auth_tries >= 5:
                    st.session_state.auth_locked_until = time.time() + 60
                    st.session_state.auth_tries = 0
                    st.warning("å·²æš‚æ—¶é”å®š 60 ç§’ã€‚")
    with c2:
        if st.button("æ¸…ç©º"):
            st.rerun()

    st.stop()

def logout_button():
    if st.button("ç™»å‡º"):
        st.session_state.auth_ok = False
        st.session_state.auth_locked_until = 0.0
        st.session_state.auth_tries = 0
        st.rerun()


# =========================================================
# Models
# =========================================================
@dataclass
class CompetitorInput:
    name_or_address: str
    notes: str
    menu_files_meta: Dict[str, Any]      # includes extracted items
    google: Dict[str, Any]
    yelp: Dict[str, Any]

@dataclass
class ReportInputs:
    report_date: str
    restaurant_cn: str
    restaurant_en: str
    address: str
    radius_miles: float

    own_menu_meta: Dict[str, Any]
    orders_meta: Dict[str, Any]

    competitors: List[CompetitorInput]
    extra_business_context: str

    acs: Optional[Dict[str, Any]]
    tract_info: Optional[Dict[str, Any]]
    restaurant_google: Dict[str, Any]

    charts: Dict[str, bytes]  # chart_name -> png bytes


# =========================================================
# Fonts
# =========================================================
def register_fonts():
    if os.path.exists(FONT_NOTO_REG):
        pdfmetrics.registerFont(TTFont("Noto", FONT_NOTO_REG))
    elif os.path.exists(FONT_NOTO_VAR):
        pdfmetrics.registerFont(TTFont("Noto", FONT_NOTO_VAR))
    if os.path.exists(FONT_NOTO_BOLD):
        pdfmetrics.registerFont(TTFont("Noto-Bold", FONT_NOTO_BOLD))

    if os.path.exists(FONT_ROBOTO_REG):
        pdfmetrics.registerFont(TTFont("Roboto", FONT_ROBOTO_REG))
    elif os.path.exists(FONT_ROBOTO_VAR):
        pdfmetrics.registerFont(TTFont("Roboto", FONT_ROBOTO_VAR))
    if os.path.exists(FONT_ROBOTO_BOLD):
        pdfmetrics.registerFont(TTFont("Roboto-Bold", FONT_ROBOTO_BOLD))
    if os.path.exists(FONT_ROBOTO_ITALIC):
        pdfmetrics.registerFont(TTFont("Roboto-Italic", FONT_ROBOTO_ITALIC))
    elif os.path.exists(FONT_ROBOTO_VAR_ITALIC):
        pdfmetrics.registerFont(TTFont("Roboto-Italic", FONT_ROBOTO_VAR_ITALIC))

def f_cn(bold=False):
    if bold and "Noto-Bold" in pdfmetrics.getRegisteredFontNames():
        return "Noto-Bold"
    if "Noto" in pdfmetrics.getRegisteredFontNames():
        return "Noto"
    return "Helvetica-Bold" if bold else "Helvetica"

def f_en(bold=False, italic=False):
    if italic and "Roboto-Italic" in pdfmetrics.getRegisteredFontNames():
        return "Roboto-Italic"
    if bold and "Roboto-Bold" in pdfmetrics.getRegisteredFontNames():
        return "Roboto-Bold"
    if "Roboto" in pdfmetrics.getRegisteredFontNames():
        return "Roboto"
    return "Helvetica-Bold" if bold else "Helvetica"

def is_ascii_line(s: str) -> bool:
    s = s.strip()
    return bool(s) and all(ord(ch) < 128 for ch in s)


# =========================================================
# Utils
# =========================================================
def draw_bg(c: canvas.Canvas, bg_path: str):
    if bg_path and os.path.exists(bg_path):
        c.drawImage(bg_path, 0, 0, width=PAGE_W, height=PAGE_H, mask="auto")

def sanitize_text(text: str) -> str:
    text = text.replace("```", "").replace("`", "")
    text = text.replace("â€¢", "-")
    # avoid markdown headings leaking
    text = re.sub(r'^\s{0,3}#{1,6}\s*', '', text, flags=re.M)
    text = re.sub(r'\*\*(.*?)\*\*', r'\1', text)
    return text.strip()

def wrap_lines_by_chars(text: str, max_chars: int) -> List[str]:
    lines: List[str] = []
    for para in text.splitlines():
        para = para.rstrip()
        if not para.strip():
            lines.append("")
            continue
        lines.extend(textwrap.wrap(para, width=max_chars, break_long_words=False, replace_whitespace=False))
    return lines

def parse_sections(text: str) -> List[Tuple[str, str]]:
    """
    æ”¯æŒä¸¤ç§æ ‡é¢˜ï¼š
    1) ã€Titleã€‘
    2) ^\d+\. Title
    """
    text = text.strip()
    if not text:
        return []

    lines = text.splitlines()
    norm = []
    for ln in lines:
        norm.append(ln.rstrip())
    text = "\n".join(norm)

    sections = []
    cur_title = None
    cur_body = []

    def flush():
        nonlocal cur_title, cur_body
        if cur_title is not None:
            sections.append((cur_title.strip(), "\n".join(cur_body).strip()))
        cur_title = None
        cur_body = []

    for ln in text.splitlines():
        ln_stripped = ln.strip()
        is_bracket = ln_stripped.startswith("ã€") and ln_stripped.endswith("ã€‘") and len(ln_stripped) >= 4
        is_num = bool(re.match(r'^\d+\.\s+\S+', ln_stripped))
        if is_bracket or is_num:
            flush()
            cur_title = ln_stripped.replace("ã€", "").replace("ã€‘", "") if is_bracket else ln_stripped
        else:
            cur_body.append(ln)

    flush()
    if len(sections) == 1 and sections[0][0] and sections[0][1] == "":
        return []
    return sections


# =========================================================
# Google Places
# =========================================================
def google_geocode(address: str, api_key: str) -> Optional[Tuple[float, float]]:
    url = "https://maps.googleapis.com/maps/api/geocode/json"
    r = requests.get(url, params={"address": address, "key": api_key}, timeout=30)
    r.raise_for_status()
    data = r.json()
    if data.get("status") != "OK" or not data.get("results"):
        return None
    loc = data["results"][0]["geometry"]["location"]
    return float(loc["lat"]), float(loc["lng"])

def google_nearby_restaurants(lat: float, lng: float, api_key: str, radius_m: int = 1200) -> List[Dict[str, Any]]:
    url = "https://maps.googleapis.com/maps/api/place/nearbysearch/json"
    results: List[Dict[str, Any]] = []
    params = {"location": f"{lat},{lng}", "radius": radius_m, "type": "restaurant", "key": api_key}
    for _ in range(3):
        r = requests.get(url, params=params, timeout=30)
        r.raise_for_status()
        data = r.json()
        if data.get("status") not in ("OK", "ZERO_RESULTS"):
            break
        results.extend(data.get("results", []))
        token = data.get("next_page_token")
        if not token:
            break
        time.sleep(2)
        params = {"pagetoken": token, "key": api_key}
    return results

def google_place_details(place_id: str, api_key: str) -> Dict[str, Any]:
    url = "https://maps.googleapis.com/maps/api/place/details/json"
    fields = ",".join([
        "name","formatted_address","rating","user_ratings_total","types",
        "url","website","formatted_phone_number","opening_hours","reviews","geometry"
    ])
    r = requests.get(url, params={"place_id": place_id, "fields": fields, "key": api_key}, timeout=30)
    r.raise_for_status()
    data = r.json()
    if data.get("status") != "OK":
        return {}
    return data.get("result", {})

def google_textsearch_place_id(query: str, api_key: str) -> Optional[str]:
    url = "https://maps.googleapis.com/maps/api/place/textsearch/json"
    r = requests.get(url, params={"query": query, "key": api_key}, timeout=30)
    r.raise_for_status()
    data = r.json()
    if data.get("status") != "OK" or not data.get("results"):
        return None
    return data["results"][0].get("place_id")


# =========================================================
# Census ACS
# =========================================================
def census_tract_from_latlng(lat: float, lng: float) -> Optional[Dict[str, str]]:
    url = "https://geocoding.geo.census.gov/geocoder/geographies/coordinates"
    params = {"x": lng, "y": lat, "benchmark": "Public_AR_Current", "vintage": "Current_Current", "format": "json"}
    r = requests.get(url, params=params, timeout=30)
    r.raise_for_status()
    data = r.json()
    try:
        tract = data["result"]["geographies"]["Census Tracts"][0]
        return {"STATE": tract["STATE"], "COUNTY": tract["COUNTY"], "TRACT": tract["TRACT"], "NAME": tract.get("NAME","")}
    except Exception:
        return None

def acs_5y_profile(state: str, county: str, tract: str, year: int = 2023) -> Optional[Dict[str, Any]]:
    vars_map = {
        "pop_total": "B01003_001E",
        "median_income": "B19013_001E",
        "median_age": "B01002_001E",
        "white": "B02001_002E",
        "black": "B02001_003E",
        "asian": "B02001_005E",
        "hispanic": "B03003_003E",
        "owner_occ": "B25003_002E",
        "renter_occ": "B25003_003E",
    }
    get_vars = ",".join(["NAME"] + list(vars_map.values()))
    url = f"https://api.census.gov/data/{year}/acs/acs5"
    params = {"get": get_vars, "for": f"tract:{tract}", "in": f"state:{state} county:{county}"}
    r = requests.get(url, params=params, timeout=30)
    r.raise_for_status()
    data = r.json()
    if not data or len(data) < 2:
        return None
    headers, values = data[0], data[1]
    row = dict(zip(headers, values))

    def to_num(x):
        try:
            return float(x)
        except Exception:
            return None

    out = {"year": year, "name": row.get("NAME",""), "state": state, "county": county, "tract": tract}
    for k, v in vars_map.items():
        out[k] = to_num(row.get(v))

    pop = out.get("pop_total") or 0.0
    if pop > 0:
        out["pct_asian"] = (out.get("asian") or 0.0) / pop
        out["pct_white"] = (out.get("white") or 0.0) / pop
        out["pct_black"] = (out.get("black") or 0.0) / pop
        out["pct_hispanic"] = (out.get("hispanic") or 0.0) / pop
    else:
        out["pct_asian"] = out["pct_white"] = out["pct_black"] = out["pct_hispanic"] = None

    owner = out.get("owner_occ") or 0.0
    renter = out.get("renter_occ") or 0.0
    occ_total = owner + renter
    out["pct_owner"] = (owner / occ_total) if occ_total > 0 else None
    out["pct_renter"] = (renter / occ_total) if occ_total > 0 else None
    return out


# =========================================================
# Yelp (optional)
# =========================================================
def yelp_search_business(term: str, location: str, api_key: str, limit: int = 3) -> Dict[str, Any]:
    url = "https://api.yelp.com/v3/businesses/search"
    headers = {"Authorization": f"Bearer {api_key}"}
    params = {"term": term, "location": location, "limit": limit}
    r = requests.get(url, headers=headers, params=params, timeout=30)
    if r.status_code != 200:
        return {"error": f"Yelp search failed: {r.status_code}", "raw": r.text[:300]}
    return r.json()

def yelp_get_reviews(business_id: str, api_key: str) -> Dict[str, Any]:
    url = f"https://api.yelp.com/v3/businesses/{business_id}/reviews"
    headers = {"Authorization": f"Bearer {api_key}"}
    r = requests.get(url, headers=headers, timeout=30)
    if r.status_code != 200:
        return {"error": f"Yelp reviews failed: {r.status_code}", "raw": r.text[:300]}
    return r.json()


# =========================================================
# OpenAI Responses API
# =========================================================
def openai_responses(api_key: str, payload: Dict[str, Any], timeout: int = 240) -> Dict[str, Any]:
    url = "https://api.openai.com/v1/responses"
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    r = requests.post(url, headers=headers, json=payload, timeout=timeout)
    r.raise_for_status()
    return r.json()

def openai_text(prompt: str, api_key: str, model: str, temperature: float = 0.25) -> str:
    payload = {"model": model, "input": prompt, "temperature": temperature}
    data = openai_responses(api_key, payload, timeout=240)
    out = []
    for item in data.get("output", []):
        for c in item.get("content", []):
            if c.get("type") == "output_text":
                out.append(c.get("text", ""))
    return "\n".join(out).strip()

def _file_to_text_summary(uploaded_file) -> str:
    name = uploaded_file.name.lower()
    raw = uploaded_file.read()
    uploaded_file.seek(0)

    if name.endswith(".txt"):
        try:
            return raw.decode("utf-8", errors="ignore")[:50000]
        except Exception:
            return str(raw[:2000])

    if name.endswith(".csv"):
        try:
            df = pd.read_csv(uploaded_file)
            uploaded_file.seek(0)
            return df.head(300).to_csv(index=False)[:50000]
        except Exception:
            uploaded_file.seek(0)
            return "CSVè¯»å–å¤±è´¥ã€‚"

    if name.endswith(".xlsx") or name.endswith(".xls"):
        try:
            df = pd.read_excel(uploaded_file)
            uploaded_file.seek(0)
            return df.head(300).to_csv(index=False)[:50000]
        except Exception:
            uploaded_file.seek(0)
            return "Excelè¯»å–å¤±è´¥ã€‚"

    return ""

def extract_menu_with_openai(files: List[Any], api_key: str, model: str, label: str) -> Dict[str, Any]:
    if not files:
        return {"label": label, "files": [], "extracted": {"note": "no files", "items": [], "promos": []}}

    extracted_items = []
    promos = []
    notes = []

    for f in files:
        fname = f.name
        lower = fname.lower()

        if lower.endswith((".png", ".jpg", ".jpeg", ".webp")):
            b = f.read()
            f.seek(0)
            b64 = base64.b64encode(b).decode("utf-8")
            mime = "image/png" if lower.endswith(".png") else "image/jpeg"
            data_url = f"data:{mime};base64,{b64}"

            prompt = (
                "ä½ æ˜¯é¤å…å¤–å–èœå•è§£æå™¨ã€‚è¯·ä»èœå•å›¾ç‰‡ä¸­è¯†åˆ«ï¼š\n"
                "1) èœå“åç§°ï¼ˆå°½é‡æŠ“ä¸­è‹±æ–‡ï¼‰\n"
                "2) ä»·æ ¼ï¼ˆä¿ç•™$æˆ–è´§å¸ç¬¦å·ï¼‰\n"
                "3) åˆ†ç±»/æ ç›®ï¼ˆå¦‚æœèƒ½æ¨æ–­ï¼‰\n"
                "4) åŠ ä»·é¡¹/å¥—é¤ç»“æ„/å¤§å°ä»½ï¼ˆå¦‚æœ‰ï¼‰\n"
                "5) ä¿ƒé”€ä¸è¥é”€æ–‡æ¡ˆï¼ˆä¹°ä¸€é€ä¸€/æ»¡å‡/æŠ˜æ‰£/å…é…é€ç­‰ï¼‰\n"
                "åªè¾“å‡ºJSONï¼Œä¸è¦è¾“å‡ºä»»ä½•é¢å¤–æ–‡å­—ã€‚\n"
                "JSONç»“æ„ï¼š"
                "{\"items\":[{\"name\":\"\",\"price\":\"\",\"category\":\"\",\"notes\":\"\"}],"
                "\"promos\":[\"\"],\"platform_hints\":[\"\"],\"quality_flags\":[\"\"]}"
            )

            payload = {
                "model": model,
                "temperature": 0.2,
                "input": [{
                    "role": "user",
                    "content": [
                        {"type": "input_text", "text": prompt},
                        {"type": "input_image", "image_url": data_url},
                    ],
                }],
            }
            try:
                resp = openai_responses(api_key, payload, timeout=240)
                text_out = ""
                for item in resp.get("output", []):
                    for c in item.get("content", []):
                        if c.get("type") == "output_text":
                            text_out += c.get("text", "")
                text_out = text_out.strip()

                m = re.search(r"\{.*\}", text_out, flags=re.S)
                if not m:
                    notes.append(f"{fname}: visionè¾“å‡ºæ— æ³•è§£æä¸ºJSONã€‚")
                    continue
                obj = json.loads(m.group(0))
                extracted_items.extend(obj.get("items", []))
                promos.extend(obj.get("promos", []))
            except Exception as e:
                notes.append(f"{fname}: visionè§£æå¤±è´¥: {str(e)[:200]}")
            continue

        if lower.endswith((".txt", ".csv", ".xlsx", ".xls")):
            try:
                text_blob = _file_to_text_summary(f)
                f.seek(0)
            except Exception:
                text_blob = ""
                try:
                    f.seek(0)
                except Exception:
                    pass

            if not text_blob.strip():
                notes.append(f"{fname}: æ— æ³•æå–æ–‡æœ¬å†…å®¹ã€‚")
                continue

            prompt = (
                "ä½ æ˜¯é¤å…å¤–å–èœå•è§£æå™¨ã€‚ä»¥ä¸‹æ˜¯èœå•æ–‡æœ¬ï¼ˆå¯èƒ½æ¥è‡ªCSV/Excel/TXTï¼‰ã€‚\n"
                "æå–ï¼šèœå“ã€ä»·æ ¼ã€åˆ†ç±»ã€åŠ ä»·é¡¹/å¥—é¤ç»“æ„ã€ä¿ƒé”€ä¿¡æ¯ã€‚\n"
                "åªè¾“å‡ºJSONï¼Œä¸è¦è¾“å‡ºä»»ä½•é¢å¤–æ–‡å­—ã€‚\n"
                "JSONç»“æ„ï¼š"
                "{\"items\":[{\"name\":\"\",\"price\":\"\",\"category\":\"\",\"notes\":\"\"}],"
                "\"promos\":[\"\"],\"platform_hints\":[\"\"],\"quality_flags\":[\"\"]}\n\n"
                f"èœå•åŸæ–‡å¼€å§‹ï¼š\n{text_blob}\nèœå•åŸæ–‡ç»“æŸã€‚"
            )
            try:
                text_out = openai_text(prompt, api_key, model=model, temperature=0.2)
                m = re.search(r"\{.*\}", text_out, flags=re.S)
                if not m:
                    notes.append(f"{fname}: æ–‡æœ¬è§£æè¾“å‡ºæ— æ³•è§£æä¸ºJSONã€‚")
                    continue
                obj = json.loads(m.group(0))
                extracted_items.extend(obj.get("items", []))
                promos.extend(obj.get("promos", []))
            except Exception as e:
                notes.append(f"{fname}: æ–‡æœ¬è§£æå¤±è´¥: {str(e)[:200]}")
            continue

        notes.append(f"{fname}: ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼ˆå»ºè®® png/jpg/txt/csv/xlsxï¼‰ã€‚")

    extracted_items = extracted_items[:1200]
    promos = promos[:150]

    return {
        "label": label,
        "files": [{"name": f.name, "type": getattr(f, "type", "")} for f in files],
        "extracted": {
            "items": extracted_items,
            "promos": promos,
            "notes": notes[:100],
        }
    }


# =========================================================
# Orders Meta
# =========================================================
def summarize_orders(files: List[Any]) -> Dict[str, Any]:
    meta = {"files": [], "note": "No uploads"}
    if not files:
        return meta
    meta["note"] = "Uploaded"
    for f in files:
        try:
            df = pd.read_csv(f)
            cols = list(df.columns)[:80]
            meta["files"].append({
                "name": getattr(f, "name", "orders.csv"),
                "rows": int(df.shape[0]),
                "cols_sample": cols,
                "date_col_guess": next((c for c in cols if "date" in c.lower() or "time" in c.lower()), None),
                "amount_col_guess": next((c for c in cols if "total" in c.lower() or "amount" in c.lower() or "subtotal" in c.lower()), None),
            })
        except Exception as e:
            meta["files"].append({"name": getattr(f, "name", "orders"), "error": str(e)[:200]})
    return meta


# =========================================================
# Menu Stats + Charts (visual)
# =========================================================
def _to_price(x: Any) -> Optional[float]:
    if x is None:
        return None
    s = str(x)
    m = re.search(r'(\d+(\.\d+)?)', s.replace(",", ""))
    if not m:
        return None
    try:
        return float(m.group(1))
    except Exception:
        return None

def menu_to_df(menu_meta: Dict[str, Any]) -> pd.DataFrame:
    items = (((menu_meta or {}).get("extracted") or {}).get("items") or [])
    rows = []
    for it in items:
        name = (it.get("name") or "").strip()
        cat = (it.get("category") or "").strip()
        price = _to_price(it.get("price"))
        notes = (it.get("notes") or "").strip()
        if not name and price is None:
            continue
        rows.append({"name": name, "category": cat if cat else "Uncategorized", "price": price, "notes": notes})
    df = pd.DataFrame(rows)
    if df.empty:
        return df
    df["category"] = df["category"].fillna("Uncategorized")
    return df

def make_png(fig) -> bytes:
    buf = io.BytesIO()
    fig.savefig(buf, format="png", dpi=180, bbox_inches="tight")
    plt.close(fig)
    buf.seek(0)
    return buf.read()

def build_charts(own_df: pd.DataFrame, comps: List[Tuple[str, pd.DataFrame]]) -> Dict[str, bytes]:
    charts: Dict[str, bytes] = {}
    if own_df is None or own_df.empty:
        return charts

    # 1) Price distribution
    fig = plt.figure()
    own_df["price"].dropna().plot(kind="hist", bins=18)
    plt.title("Own Menu Price Distribution")
    plt.xlabel("Price")
    plt.ylabel("Count")
    charts["chart_own_price_hist"] = make_png(fig)

    # 2) Category count
    fig = plt.figure()
    own_df["category"].value_counts().head(12).plot(kind="bar")
    plt.title("Own Menu Category Mix (Top 12)")
    plt.xlabel("Category")
    plt.ylabel("Items")
    charts["chart_own_category_bar"] = make_png(fig)

    # 3) Price tiers matrix (count by tier)
    bins = [0, 8, 12, 16, 20, 25, 35, 999]
    labels = ["<$8", "$8-12", "$12-16", "$16-20", "$20-25", "$25-35", "$35+"]
    tier = pd.cut(own_df["price"], bins=bins, labels=labels, include_lowest=True)
    fig = plt.figure()
    tier.value_counts().reindex(labels).fillna(0).plot(kind="bar")
    plt.title("Own Menu Price Tiers")
    plt.xlabel("Tier")
    plt.ylabel("Items")
    charts["chart_own_price_tiers"] = make_png(fig)

    # 4) Competitor median price compare (if any comp has prices)
    comp_rows = []
    for name, dfc in comps:
        if dfc is not None and not dfc.empty and dfc["price"].dropna().shape[0] >= 5:
            comp_rows.append({"competitor": name, "median_price": float(dfc["price"].median())})
    if comp_rows:
        comp_df = pd.DataFrame(comp_rows).sort_values("median_price")
        fig = plt.figure()
        comp_df.set_index("competitor")["median_price"].plot(kind="bar")
        plt.title("Competitor Median Price (from uploaded menus)")
        plt.xlabel("Competitor")
        plt.ylabel("Median Price")
        charts["chart_comp_median_price"] = make_png(fig)

    return charts


# =========================================================
# Menu Smart Adjust (NEW)
# =========================================================
DEFAULT_TARGET_MEDIAN_MAIN = 18.95
HIGH_PRICE_THRESHOLD_RATIO = 1.12
MIN_COMBOS_REQUIRED = 4
FRONT_LIMIT = 35

def _has_chinese(s: str) -> bool:
    return any("\u4e00" <= ch <= "\u9fff" for ch in (s or ""))

def _split_cn_en(name: str) -> Tuple[str, str]:
    """
    å°è¯•ä» name ä¸­æ‹†å‡ºä¸­è‹±æ–‡ã€‚æ— æ³•æ‹†å°±ï¼šä¸­æ–‡/è‹±æ–‡éƒ½å¡« nameï¼ˆä¸å¼ºè¡ŒçŒœï¼‰ã€‚
    """
    name = (name or "").strip()
    if not name:
        return "", ""
    if _has_chinese(name) and re.search(r"[A-Za-z]", name):
        # ç²—æš´æ‹†ï¼šæŠŠä¸­æ–‡å’Œè‹±æ–‡å„è‡ªæå–
        cn = "".join([ch for ch in name if "\u4e00" <= ch <= "\u9fff" or ch in "ï¼ˆï¼‰()Â·â€¢- "]).strip()
        en = re.sub(r"[\u4e00-\u9fffï¼ˆï¼‰()Â·â€¢]", " ", name)
        en = re.sub(r"\s+", " ", en).strip()
        return cn, en
    # ä»…ä¸­æ–‡æˆ–ä»…è‹±æ–‡
    if _has_chinese(name):
        return name, ""
    return "", name

def normalize_extracted_items(menu_meta: Dict[str, Any]) -> List[Dict[str, Any]]:
    rows = (((menu_meta or {}).get("extracted") or {}).get("items") or [])
    out = []
    seen = set()
    for it in rows:
        name = (it.get("name") or "").strip()
        if not name:
            continue
        cn, en = _split_cn_en(name)
        price = _to_price(it.get("price"))
        cat = (it.get("category") or "").strip()
        notes = (it.get("notes") or "").strip()

        key = (cn.lower(), en.lower(), price, cat.lower())
        if key in seen:
            continue
        seen.add(key)

        out.append({
            "name_cn": cn,
            "name_en": en,
            "price": price,
            "raw_category": cat if cat else "Uncategorized",
            "notes": notes,
        })
    return out

def item_fullname(it: Dict[str, Any]) -> str:
    cn = (it.get("name_cn") or "").strip()
    en = (it.get("name_en") or "").strip()
    if cn and en:
        return f"{en} {cn}".strip()
    return cn or en

def classify_item(it: Dict[str, Any]) -> str:
    name = (item_fullname(it) or "").lower()
    cat = (it.get("raw_category") or "").lower()
    notes = (it.get("notes") or "").lower()
    blob = " ".join([name, cat, notes])

    # Combo
    if any(k in blob for k in ["å¥—é¤", "è¶…å€¼", "combo", "value", "set", "bundle"]):
        return "combo"
    # Add-on
    if any(k in blob for k in ["åŠ ", "åŠ é…", "å‡çº§", "upgrade", "add-on", "addon", "extra", "+$"]):
        return "addon"
    # Drink
    if any(k in blob for k in [
        "å¥¶èŒ¶","milk tea","æŸ æª¬","lemon","coffee","å’–å•¡","soda","æ±½æ°´","coconut","æ¤°å­",
        "jasmine","èŒ‰è‰","chrysanthemum","èŠèŠ±","prunella","å¤æ¯è‰","tea","èŒ¶"
    ]):
        return "drink"
    # Main (rough)
    if any(k in blob for k in ["é¥­","rice","æ²³ç²‰","chow fun","ç‚’é¢","noodle","æ„ç²‰","spaghetti","ç²‰","é¢","ç„—","baked"]):
        return "main"
    return "other"

def split_by_type(items: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
    out = {"main": [], "drink": [], "combo": [], "addon": [], "other": []}
    for it in items:
        t = classify_item(it)
        it["type"] = t
        out[t].append(it)
    return out

def median_price(items: List[Dict[str, Any]]) -> Optional[float]:
    prices = [it["price"] for it in items if isinstance(it.get("price"), (int, float)) and it["price"] and it["price"] > 0]
    if not prices:
        return None
    s = sorted(prices)
    n = len(s)
    if n % 2 == 1:
        return s[n // 2]
    return (s[n // 2 - 1] + s[n // 2]) / 2.0

def detect_combo_system(combos: List[Dict[str, Any]], min_required: int) -> bool:
    return len(combos) >= min_required

def is_menu_overpriced(mains: List[Dict[str, Any]], target_median: float, ratio: float) -> Tuple[bool, Optional[float]]:
    cur = median_price(mains)
    if cur is None:
        return False, None
    return (cur > target_median * ratio), cur

def scale_prices(items: List[Dict[str, Any]], scale: float) -> List[Dict[str, Any]]:
    adjusted = []
    for it in items:
        p = it.get("price")
        it2 = dict(it)
        it2["price_old"] = p
        if isinstance(p, (int, float)) and p > 0:
            it2["price"] = round(float(p) * scale, 2)
        adjusted.append(it2)
    return adjusted

def safe_json_obj(text: str) -> Dict[str, Any]:
    if not text:
        return {}
    text = text.strip()
    try:
        return json.loads(text)
    except Exception:
        pass
    m = re.search(r"\{[\s\S]*\}", text)
    if not m:
        return {}
    try:
        return json.loads(m.group(0))
    except Exception:
        return {}

def ai_market_benchmark(
    items: List[Dict[str, Any]],
    location: str,
    cuisine: str,
    target_median_main: float,
    api_key: str,
    model: str,
) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    """
    ç”¨ AI ä¼°ç®—åŒåœ°åŒºåŒå“ç±»â€œä¸»é£Ÿåˆç†ä»·å¸¦â€ï¼Œå¹¶ç»™å‡ºé€èœ new_priceã€‚
    å¤±è´¥åˆ™è¿”å›åŸ items + meta(ai_error)ã€‚
    """
    mains = [i for i in items if classify_item(i) == "main" and isinstance(i.get("price"), (int, float)) and i["price"] and i["price"] > 0]
    cur_med = median_price(mains) or 0

    prompt = f"""
ä½ æ˜¯åŒ—ç¾å¤–å–å®šä»·ä¸èœå•å·¥ç¨‹é¡¾é—®ã€‚
åœ°ç‚¹ï¼š{location}
èœç³»ï¼š{cuisine}

æˆ‘ä¼šç»™ä½ ä¸€ä»½èœå“åˆ—è¡¨ï¼ˆå«ä»·æ ¼ï¼‰ã€‚è¯·å®Œæˆï¼š
1) ç»™å‡ºåŒåœ°åŒºåŒå“ç±»å¤–å–å¹³å°â€œä¸»é£Ÿâ€å¸¸è§åˆç†ä¸­ä½æ•°å’Œåˆç†åŒºé—´ï¼ˆèŒƒå›´è¦å¯æ‰§è¡Œï¼‰ã€‚
2) åˆ¤æ–­æ•´ä½“æ˜¯å¦åé«˜ã€‚
3) å¦‚æœåé«˜ï¼Œç»™å‡ºæ¯ä¸€é“èœå»ºè®®çš„æ–°ä»·æ ¼ï¼ˆåªè¦ç»™å‡ºä½ æœ‰æŠŠæ¡çš„ï¼Œæ²¡æŠŠæ¡å¯ä»¥ä¸å†™åœ¨ adjusted_items é‡Œï¼‰ã€‚

åªè¾“å‡ºä¸¥æ ¼ JSONï¼Œä¸è¦è¾“å‡ºä»»ä½•é¢å¤–æ–‡å­—ï¼š
{{
  "market_median_main": 0,
  "market_range_main": [0,0],
  "is_overpriced": true/false,
  "strategy": "ä¸€å¥è¯ç­–ç•¥",
  "adjusted_items": [
    {{"name":"", "new_price":0}}
  ]
}}

å½“å‰ç›®æ ‡ä¸»é£Ÿä¸­ä½æ•°å€¾å‘ï¼š{target_median_main}
å½“å‰ä¸»é£Ÿä¸­ä½æ•°ï¼ˆä¼°ç®—ï¼‰ï¼š{cur_med}

èœå“ï¼ˆname - priceï¼‰ï¼š
{json.dumps([{"name": item_fullname(i), "price": i.get("price")} for i in items], ensure_ascii=False)}
""".strip()

    try:
        text_out = openai_text(prompt, api_key, model=model, temperature=0.2)
        obj = safe_json_obj(text_out)
        mapping = {x.get("name", ""): x.get("new_price") for x in (obj.get("adjusted_items") or [])}

        adjusted = []
        for it in items:
            nm = item_fullname(it)
            newp = mapping.get(nm, None)
            it2 = dict(it)
            it2["price_old"] = it.get("price")
            if isinstance(newp, (int, float)) and newp > 0:
                it2["price"] = round(float(newp), 2)
            adjusted.append(it2)
        return adjusted, obj
    except Exception as e:
        return items, {"ai_error": str(e)[:200]}

def build_combos_if_missing(
    items: List[Dict[str, Any]],
    location: str,
    cuisine: str,
    api_key: str,
    model: str,
) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    typed = split_by_type(items)
    mains = [m for m in typed["main"] if isinstance(m.get("price"), (int, float)) and m["price"] and m["price"] > 0]
    drinks = [d for d in typed["drink"] if isinstance(d.get("price"), (int, float)) and d["price"] and d["price"] > 0]

    # pick candidates
    mains_sorted = sorted(mains, key=lambda x: abs((x.get("price") or 0) - DEFAULT_TARGET_MEDIAN_MAIN))
    mains_pick = mains_sorted[:6]
    drinks_pick = drinks[:4]

    new_combos = []
    for i, main in enumerate(mains_pick):
        if not drinks_pick:
            break
        drink = drinks_pick[i % len(drinks_pick)]
        base = (main["price"] or 0) + (drink["price"] or 0)
        combo_price = round(base - 1.50, 2)

        combo_name_cn = f"è¶…å€¼å¥—é¤ï½œ{(main.get('name_cn') or 'ä¸»é£Ÿ')} + {(drink.get('name_cn') or 'é¥®å“')}"
        combo_name_en = f"Value Combo | {(main.get('name_en') or 'Main')} + {(drink.get('name_en') or 'Drink')}"
        new_combos.append({
            "name_cn": combo_name_cn,
            "name_en": combo_name_en,
            "price": combo_price,
            "raw_category": "Value Combos",
            "notes": "Auto-generated combo",
            "type": "combo",
        })

    meta = {"generated_combos": len(new_combos), "ai_combo_optimized": False}

    # AI optimize combo names/prices (best effort)
    if api_key:
        try:
            prompt = f"""
ä½ æ˜¯å¤–å–å¥—é¤å·¥ç¨‹å¸ˆã€‚åœ°ç‚¹ï¼š{location}ï¼Œèœç³»ï¼š{cuisine}
æˆ‘ç”Ÿæˆäº†ä¸€æ‰¹å¥—é¤ï¼ˆä¸»é£Ÿ+é¥®å“ï¼‰ã€‚è¯·ä¼˜åŒ–ï¼š
1) å¥—é¤å‘½åï¼šæ›´åƒå¹³å°çˆ†å“ï¼Œç®€çŸ­ã€æ˜ç¡®ã€åˆ©äºç‚¹å‡»
2) å¥—é¤ä»·æ ¼ï¼šä¿æŒâ€œæ¯”å•ç‚¹æ›´åˆ’ç®—â€çš„æ„ŸçŸ¥ï¼›ä¸è¦ä½åˆ°ä¸èµšé’±
åªè¾“å‡ºä¸¥æ ¼JSONï¼š
{{
  "combos":[{{"idx":0,"name_cn":"","name_en":"","price":0}}]
}}
å½“å‰å¥—é¤ï¼š
{json.dumps(new_combos, ensure_ascii=False)}
""".strip()
            out = openai_text(prompt, api_key, model=model, temperature=0.2)
            obj = safe_json_obj(out)
            combos_out = obj.get("combos") or []
            for row in combos_out:
                idx = row.get("idx")
                if isinstance(idx, int) and 0 <= idx < len(new_combos):
                    if row.get("name_cn"): new_combos[idx]["name_cn"] = row["name_cn"]
                    if row.get("name_en"): new_combos[idx]["name_en"] = row["name_en"]
                    if isinstance(row.get("price"), (int, float)) and row["price"] > 0:
                        new_combos[idx]["price"] = round(float(row["price"]), 2)
            meta["ai_combo_optimized"] = True
        except Exception as e:
            meta["ai_error"] = str(e)[:200]

    merged = items + new_combos
    # re-dedup
    seen = set()
    dedup = []
    for it in merged:
        key = (item_fullname(it).lower(), it.get("price"), (it.get("raw_category") or "").lower())
        if key in seen:
            continue
        seen.add(key)
        dedup.append(it)
    return dedup, meta

def build_smart_menu(items: List[Dict[str, Any]], front_limit: int = FRONT_LIMIT) -> Dict[str, Any]:
    typed = split_by_type(items)
    mains = [m for m in typed["main"] if isinstance(m.get("price"), (int, float)) and m["price"] and m["price"] > 0]
    drinks = [d for d in typed["drink"] if isinstance(d.get("price"), (int, float)) and d["price"] and d["price"] > 0]
    combos = [c for c in typed["combo"] if isinstance(c.get("price"), (int, float)) and c["price"] and c["price"] > 0]
    addons = [a for a in typed["addon"] if isinstance(a.get("price"), (int, float)) and a["price"] and a["price"] > 0]
    others = typed["other"]

    anchors = sorted(mains, key=lambda x: x["price"], reverse=True)[:6]

    core_band = [m for m in mains if 16.0 <= float(m["price"]) <= 20.5 and m not in anchors]
    best_sellers = sorted(core_band, key=lambda x: abs(float(x["price"]) - DEFAULT_TARGET_MEDIAN_MAIN))[:10]

    front_combos = combos[:8]
    front_addons = addons[:6]
    front_drinks = drinks[:6]

    used = set()
    def mark_used(lst):
        for x in lst:
            used.add((item_fullname(x), x.get("price")))

    mark_used(anchors)
    mark_used(best_sellers)
    mark_used(front_combos)
    mark_used(front_addons)
    mark_used(front_drinks)

    classic = []
    for it in items:
        if (item_fullname(it), it.get("price")) not in used:
            classic.append(it)

    return {
        "Chef_Signature": anchors,
        "Best_Sellers": best_sellers,
        "Value_Combos": front_combos,
        "Make_It_Better": front_addons,
        "Drinks": front_drinks,
        "Classic_Menu": classic,
    }

def menu_to_flat_df(menu: Dict[str, Any]) -> pd.DataFrame:
    rows = []
    for section, lst in menu.items():
        for it in lst:
            rows.append({
                "section": section,
                "name_cn": it.get("name_cn",""),
                "name_en": it.get("name_en",""),
                "name": item_fullname(it),
                "price": it.get("price"),
                "price_old": it.get("price_old", None),
                "raw_category": it.get("raw_category",""),
                "type": it.get("type", classify_item(it)),
                "notes": it.get("notes",""),
            })
    return pd.DataFrame(rows)


# =========================================================
# Prompt (forces deep + specific execution)
# =========================================================
def build_prompt(inputs: ReportInputs) -> str:
    own_df = menu_to_df(inputs.own_menu_meta)
    own_stats = {}
    if not own_df.empty:
        own_stats = {
            "items_count": int(own_df.shape[0]),
            "price_count": int(own_df["price"].dropna().shape[0]),
            "median_price": None if own_df["price"].dropna().empty else float(own_df["price"].median()),
            "min_price": None if own_df["price"].dropna().empty else float(own_df["price"].min()),
            "max_price": None if own_df["price"].dropna().empty else float(own_df["price"].max()),
            "top_categories": own_df["category"].value_counts().head(10).to_dict(),
        }

    blob = {
        "report_date": inputs.report_date,
        "restaurant": {
            "cn": inputs.restaurant_cn,
            "en": inputs.restaurant_en,
            "address": inputs.address,
            "radius_miles": inputs.radius_miles,
            "google": inputs.restaurant_google,
        },
        "trade_area": {
            "tract_info": inputs.tract_info,
            "acs": inputs.acs,
            "assumption_note": "ACS ä¸º tract çº§åˆ«ä»£ç†ï¼Œä½œä¸ºå•†åœˆç”»åƒçš„æ–¹å‘æ€§å‚è€ƒï¼›æŠ¥å‘Šå¿…é¡»æ˜ç¡®è¿™ä¸ªå‡è®¾å¹¶ç»™å‡ºé£é™©æç¤ºã€‚"
        },
        "own_menu": inputs.own_menu_meta,
        "own_menu_stats": own_stats,
        "orders_meta": inputs.orders_meta,
        "competitors": [
            {
                "name_or_address": c.name_or_address,
                "notes": c.notes,
                "google": c.google,
                "yelp": c.yelp,
                "menu": c.menu_files_meta,
            } for c in inputs.competitors
        ],
        "extra_business_context": inputs.extra_business_context,
        "charts_available": list(inputs.charts.keys()),
        "current_date": dt.datetime.now().strftime("%Y-%m-%d"),
        "season_hint": "è¯·æŒ‰åŒ—ç¾å¸¸è§å­£èŠ‚ï¼ˆWinter/Spring/Summer/Fallï¼‰ä¸èŠ‚æ—¥èŠ‚ç‚¹ï¼ˆåœ£è¯/æ–°å¹´/æƒ…äººèŠ‚/å¤æ´»èŠ‚/æ¯äº²èŠ‚/æš‘æœŸ/è¿”æ ¡/ä¸‡åœ£èŠ‚/æ„Ÿæ©èŠ‚ï¼‰ç»™å…·ä½“å­£èŠ‚èœå»ºè®®ã€‚",
    }

    return f"""
ä½ æ˜¯ AuraInsight çš„åŒ—ç¾é¤é¥®å¢é•¿å’¨è¯¢é¡¾é—®ï¼ˆåå¤–å– + å•†åœˆå¢é•¿ + èœå•å·¥ç¨‹ + ä¿ƒé”€ç»æµå­¦ï¼‰ã€‚
ä½ å°†æ”¶åˆ°ä¸€ä¸ª JSON æ•°æ®åŒ…ï¼ˆå«ï¼šå•†åœˆACSã€é—¨åº—ä¸ç«å¯¹çš„Google/Yelpä¿¡æ¯ã€é—¨åº—ä¸ç«å¯¹çš„å¤–å–èœå•è¯†åˆ«ç»“æœã€è®¢å•æŠ¥è¡¨å­—æ®µæ‘˜è¦ã€ä»¥åŠå·²ç»ç”Ÿæˆçš„å¯è§†åŒ–å›¾è¡¨åˆ—è¡¨ï¼‰ã€‚

ç›®æ ‡ï¼šè¾“å‡ºä¸€ä»½â€œèƒ½ç›´æ¥æ‰§è¡Œâ€çš„å’¨è¯¢çº§ã€ŠTrade Area & Growth Diagnosticã€‹æŠ¥å‘Šæ­£æ–‡ï¼Œç”¨äºç”Ÿæˆ PDFã€‚

ç¡¬æ€§è¦æ±‚ï¼ˆè¿åä»»ä½•ä¸€æ¡éƒ½ç®—å¤±è´¥ï¼‰ï¼š
1) ä¸¥ç¦è¾“å‡º Markdownï¼ˆä¸è¦å‡ºç°ï¼š#ã€##ã€**ã€```ã€|---|ã€[]()ï¼‰ã€‚
2) ç« èŠ‚æ ‡é¢˜å¿…é¡»ä½¿ç”¨ â€œ1. â€è¿™ç§ç¼–å·æ ‡é¢˜ï¼ˆä¾‹å¦‚ï¼š5. Pricing, Anchors & Promo Economicsï¼‰ã€‚
3) æ¯ä¸€ç« å¼€å¤´å¿…é¡»ç»™ 3â€“6 æ¡ Key Takeawaysï¼ˆå°½é‡å¸¦æ•°å­—ã€èŒƒå›´æˆ–æ˜ç¡®é˜ˆå€¼ï¼‰ã€‚
4) æ‰€æœ‰å»ºè®®å¿…é¡»åŒ…å«äº”ä»¶å¥—ï¼šã€åŠ¨ä½œã€‘ã€åŸå› ã€‘ã€é¢„æœŸå½±å“ã€‘ã€KPIã€‘ã€ä¸¤å‘¨éªŒè¯æ–¹æ³•ã€‘ã€‚
5) â€œç¬¬5ç« â€å¿…é¡»å†™å¾—æç»†ï¼šå¯¹ä»·æ ¼é”šç‚¹ã€æŠ˜æ‰£ã€ä¹°ä¸€é€ä¸€ã€æ»¡å‡ã€åŠ ä»·è´­ã€ç»„åˆå¥—é¤ã€å…è¿è´¹å¯¹æ ‡ç­–ç•¥é€æ¡æ‹†è§£ã€‚
   - ä¹°ä¸€é€ä¸€å¿…é¡»æŒ‡å®šåˆ°å…·ä½“èœå“ï¼ˆè‡³å°‘ 3 ä¸ªå€™é€‰ï¼‰ï¼Œå¹¶å†™æ¸…æ¥šï¼šä¸ºä»€ä¹ˆæ˜¯å®ƒã€é£é™©æ˜¯ä»€ä¹ˆã€å¦‚ä½•é™åˆ¶è§„åˆ™ã€å¦‚ä½•é˜²è–…ç¾Šæ¯›ã€å¦‚ä½•éªŒè¯ã€‚
6) å¿…é¡»å¯¹â€œå…¨èœå•æ‰€æœ‰èœå“â€åšç»“æ„åŒ–åˆ†æï¼ˆå¦‚æœè¯†åˆ«ä¸å…¨ä¹Ÿè¦åšï¼šæ ‡å‡ºç¼ºå£å¦‚ä½•è¡¥é½ï¼‰ã€‚
   - è‡³å°‘è¾“å‡ºï¼šä»·æ ¼å¸¦çŸ©é˜µï¼ˆ<$10ã€$10-15ã€$15-20ã€$20-25ã€$25+ï¼‰ï¼Œæ¯ä¸ªä»·æ ¼å¸¦åº”è¯¥æ”¾ä»€ä¹ˆå“ç±»ä¸é”šç‚¹å•†å“ã€‚
   - è‡³å°‘è¾“å‡ºï¼šèœå•å·¥ç¨‹ï¼ˆæ˜Ÿ/ç‰›/è°œ/ç‹—ï¼‰æ‰§è¡Œæ–¹å¼ä¸è½åœ°æ¸…å•ã€‚
7) è¿è¥åŠ¨ä½œæ¸…å•å¿…é¡»å±•å¼€ä¸ºâ€œå…·ä½“æ–¹æ¡ˆâ€ï¼šå­£èŠ‚æ€§ä¸Šæ–°å¿…é¡»æŒ‰â€œå½“ä¸‹å­£èŠ‚ + ä¸‹ä¸ªå­£èŠ‚â€åˆ†åˆ«ç»™å»ºè®®ï¼ˆæ¯å­£è‡³å°‘ 3 ä¸ªèœï¼‰ï¼Œå†™ï¼šä¸Šä»€ä¹ˆã€ä¸ºä»€ä¹ˆã€æ€ä¹ˆæ‹å›¾ã€æ€ä¹ˆå‘½åã€æ€ä¹ˆå®šä»·ã€æ€ä¹ˆä¸Šå¹³å°ã€æ€ä¹ˆåšé¦–å‘¨ä¿ƒé”€ã€‚
8) æŠ¥å‘Šå¿…é¡»è¶³å¤Ÿé•¿ï¼Œç”Ÿæˆå PDF ç›®æ ‡è‡³å°‘ 6â€“7 é¡µã€‚å†…å®¹è¦â€œå¯æ‰§è¡Œã€å¸¦è¡¨æ ¼/æ¸…å•â€ï¼Œä½†ä¸è¦ç”¨ Markdown è¡¨æ ¼ï¼›å¦‚æœéœ€è¦è¡¨æ ¼ï¼Œè¯·ç”¨ï¼š
   è¡¨æ ¼:
   colA,colB,colC
   ...
9) å¿…é¡»åŒ…å«â€œå¯è§†åŒ–å›¾è¡¨è§£è¯»â€ï¼š
   - ä½ ä¸ç”¨ç”»å›¾ï¼Œä½†å¿…é¡»å¼•ç”¨å›¾è¡¨åç§°å¹¶è§£è¯»ï¼ˆä¾‹å¦‚ chart_own_price_histã€chart_own_category_barã€chart_own_price_tiersã€chart_comp_median_priceï¼‰ã€‚
10) å¿…é¡»åŒ…å«â€œData Gaps & How to Collectâ€ï¼ŒæŠŠç¼ºå“ªäº›æ•°æ®ã€æ€ä¹ˆè¡¥ã€è°è´Ÿè´£ã€éœ€è¦å¤šä¹…å†™æ¸…æ¥šã€‚

è¾“å‡ºç« èŠ‚é¡ºåºå¿…é¡»å¦‚ä¸‹ï¼ˆæ ‡é¢˜ä¸€å­—ä¸å·®ï¼‰ï¼š
1. Executive Summary
2. Trade Area & Demographics
3. Customer Segments & JTBD
4. Demand, Occasion & Menu Positioning
5. Competitive Landscape (Google + Yelp + Menu)
6. Pricing, Anchors & Promo Economics
7. Menu Architecture & Menu Engineering
8. Platform Growth Playbook (30/60/90)
9. Measurement System & Experiment Design
10. Appendix A: Own Menu Deep Dive
11. Appendix B: Competitor Menu Deep Dive
12. Data Gaps & How to Collect

è¾“å…¥ JSONï¼š
{json.dumps(blob, ensure_ascii=False, indent=2)}

å¼€å§‹è¾“å‡ºæŠ¥å‘Šæ­£æ–‡ï¼š
""".strip()

def ensure_long_enough(report_text: str, api_key: str, model: str, min_chars: int = 16000) -> str:
    t = sanitize_text(report_text)
    if len(t) >= min_chars:
        return t

    expand_prompt = f"""
ä½ å°†æ”¶åˆ°ä¸€ä»½æŠ¥å‘Šæ­£æ–‡ã€‚è¯·åœ¨ä¸æ”¹å˜ç« èŠ‚æ ‡é¢˜é¡ºåºçš„å‰æä¸‹ï¼Œæ˜¾è‘—æ‰©å†™ï¼Œä½¿å…¶æ›´ç»†ã€æ›´èƒ½æ‰§è¡Œã€‚
é‡ç‚¹æ‰©å†™ï¼š
- ç¬¬6ç« ï¼ˆPricing, Anchors & Promo Economicsï¼‰ï¼šä¹°ä¸€é€ä¸€/æ»¡å‡/å¥—é¤/åŠ ä»·è´­/æŠ˜æ‰£é—¨æ§›è¦ç»™å…·ä½“èœå“ä¸è§„åˆ™ã€‚
- ç¬¬7ç« ï¼ˆMenu Engineeringï¼‰ï¼šæŠŠæ‰€æœ‰å“ç±»æŒ‰æ˜Ÿ/ç‰›/è°œ/ç‹—çš„æ‰§è¡Œæ–¹æ³•å†™æˆâ€œåŠ¨ä½œæ¸…å•â€ï¼Œå«KPIä¸ä¸¤å‘¨éªŒè¯ã€‚
- Appendix A/Bï¼šåˆ—å‡ºæ›´å¤šèœå•çº§åˆ«çš„å»ºè®®ï¼ˆè‡³å°‘ 25 æ¡ï¼‰ï¼Œæ¯æ¡å†™â€œå½“å‰ä»·(è‹¥ç¼ºå†™å¾…è¡¥é½)/å»ºè®®ä»·/ç†ç”±/ç«å¯¹å¯¹æ ‡/é£é™©ä¸å¯¹ç­–/éªŒè¯â€ã€‚
ä¸¥ç¦è¾“å‡ºMarkdownã€‚åªè¾“å‡ºå®Œæ•´æ­£æ–‡ï¼ˆåŒ…å«æ‰€æœ‰ç« èŠ‚ï¼‰ã€‚
åŸæ–‡å¼€å§‹ï¼š
{t}
åŸæ–‡ç»“æŸã€‚
""".strip()

    try:
        t2 = openai_text(expand_prompt, api_key, model=model, temperature=0.25)
        t2 = sanitize_text(t2)
        if len(t2) > len(t):
            t = t2
    except Exception:
        pass
    return t


# =========================================================
# PDF Render (fix truncation + add chart pages)
# =========================================================
def draw_footer(c: canvas.Canvas, report_date: str, page_num: int):
    c.setFillColor(colors.HexColor("#7A7A7A"))
    c.setFont(f_en(False), 8)
    c.drawString(0.75 * inch, 0.55 * inch, f"Confidential | Generated by AuraInsight | {report_date}")
    c.drawRightString(PAGE_W - 0.75 * inch, 0.55 * inch, f"Page {page_num}")
    c.setFillColor(colors.black)

def render_pdf(report_text: str, inputs: ReportInputs) -> str:
    register_fonts()
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    safe_name = "".join([ch if ch.isalnum() or ch in ("_", "-", " ") else "_" for ch in inputs.restaurant_en]).strip()
    safe_name = safe_name.replace(" ", "_") or "Restaurant"
    filename = f"AuraInsight_{safe_name}_{inputs.report_date.replace('/','-')}.pdf"
    out_path = os.path.join(OUTPUT_DIR, filename)

    c = canvas.Canvas(out_path, pagesize=letter)

    # Cover
    draw_bg(c, BG_COVER)
    c.setFillColor(colors.HexColor("#111111"))

    y_base = 210
    c.setFont(f_cn(True), 16)
    c.drawCentredString(PAGE_W / 2, y_base, inputs.restaurant_cn or inputs.restaurant_en)
    c.setFont(f_en(False), 12)
    c.setFillColor(colors.HexColor("#333333"))
    c.drawCentredString(PAGE_W / 2, y_base - 22, inputs.restaurant_en)
    c.setFont(f_en(False), 10)
    c.drawCentredString(PAGE_W / 2, y_base - 42, inputs.address)
    c.setFont(f_en(False), 10)
    c.drawCentredString(PAGE_W / 2, y_base - 62, inputs.report_date)
    c.showPage()

    # Content pages
    page_num = 1
    draw_bg(c, BG_CONTENT)

    left = 0.90 * inch
    top = PAGE_H - 1.55 * inch
    y = top - 0.45 * inch

    body_font_size = 10
    line_gap = 14
    para_gap = 10
    heading_gap = 18

    bottom_margin = 1.35 * inch

    def new_page():
        nonlocal y, page_num
        draw_footer(c, inputs.report_date, page_num)
        c.showPage()
        page_num += 1
        draw_bg(c, BG_CONTENT)
        y = top - 0.45 * inch

    def draw_heading(title: str):
        nonlocal y
        if y < (bottom_margin + 40):
            new_page()
        c.setFillColor(colors.black)
        font = f_cn(True) if any("\u4e00" <= ch <= "\u9fff" for ch in title) else f_en(True)
        c.setFont(font, 13)
        c.drawString(left, y, title[:140])
        y -= heading_gap

    def draw_body(text: str):
        nonlocal y
        max_chars = 100
        for line in wrap_lines_by_chars(text, max_chars):
            if y < bottom_margin:
                new_page()
            font = f_en(False) if is_ascii_line(line) else f_cn(False)
            c.setFillColor(colors.black)
            c.setFont(font, body_font_size)
            c.drawString(left, y, line)
            y -= line_gap
        y -= para_gap

    sections = parse_sections(report_text)
    if not sections:
        draw_body(report_text)
    else:
        for title, body in sections:
            draw_heading(title)
            if body.strip():
                draw_body(body)

    # Chart pages
    if inputs.charts:
        for chart_name, png_bytes in inputs.charts.items():
            new_page()
            draw_bg(c, BG_CONTENT)

            c.setFont(f_en(True), 13)
            c.setFillColor(colors.black)
            c.drawString(left, top - 0.25 * inch, f"Chart: {chart_name}")

            img = ImageReader(io.BytesIO(png_bytes))
            img_w = PAGE_W - 2.0 * inch
            img_h = PAGE_H - 3.0 * inch
            c.drawImage(img, left, 1.4 * inch, width=img_w, height=img_h, preserveAspectRatio=True, anchor='c')

    draw_footer(c, inputs.report_date, page_num)
    c.save()
    return out_path


# =========================================================
# UI
# =========================================================
require_login()

google_key = st.secrets.get("GOOGLE_MAPS_API_KEY", "")
openai_key = st.secrets.get("OPENAI_API_KEY", "")
yelp_key = st.secrets.get("YELP_API_KEY", "")  # optional

with st.sidebar:
    st.header("é…ç½®")
    model = st.selectbox("OpenAI æ¨¡å‹", ["gpt-4o-mini", "gpt-4.1-mini", "gpt-4o"], index=0)

    show_advanced = st.checkbox("æ˜¾ç¤ºé«˜çº§è®¾ç½®", value=False)
    if show_advanced:
        radius_miles = st.slider("å•†åœˆåŠå¾„ï¼ˆmilesï¼‰", 1.0, 6.0, 4.0, 0.5)
        nearby_radius_m = st.slider("Google Nearby æœç´¢åŠå¾„ï¼ˆç±³ï¼‰", 300, 3000, 1200, 100)
    else:
        radius_miles = 4.0
        nearby_radius_m = 1200

    st.divider()
    logout_button()
    st.divider()
    st.caption("Built by c8geek")
    st.markdown("[LinkedIn](https://www.linkedin.com/)")

st.title(APP_TITLE)

if not google_key:
    st.warning("æœªæ£€æµ‹åˆ° GOOGLE_MAPS_API_KEYï¼Œè¯·åœ¨ .streamlit/secrets.toml é…ç½®ã€‚")
if not openai_key:
    st.warning("æœªæ£€æµ‹åˆ° OPENAI_API_KEYï¼Œè¯·åœ¨ .streamlit/secrets.toml é…ç½®ã€‚")
if not yelp_key:
    st.info("æœªæ£€æµ‹åˆ° YELP_API_KEYï¼ˆå¯é€‰ï¼‰ã€‚ç«å¯¹ Yelp ç»´åº¦ä¼šç¼ºå¤±ã€‚")

# =========================================================
# Tabs: Report vs Menu Smart Adjust
# =========================================================
tab_report, tab_menu = st.tabs(["ğŸ“Š é—¨åº—åˆ†ææŠ¥å‘Šï¼ˆPDFï¼‰", "ğŸ§  èœå•æ™ºèƒ½è°ƒæ•´ï¼ˆå…ˆè°ƒä»·â†’å†ç»„å¥—é¤â†’å‡ºå®Œæ•´èœå•ï¼‰"])


# =========================================================
# TAB 1: REPORT (your original flow, unchanged)
# =========================================================
with tab_report:

    # =========================================================
    # Step 1: Search restaurant
    # =========================================================
    st.subheader("Step 1ï½œè¾“å…¥åœ°å€ â†’ æœç´¢é™„è¿‘é¤å…")
    address_input = st.text_input("è¾“å…¥åœ°å€ï¼ˆç”¨äºå®šä½å¹¶æœç´¢é™„è¿‘é¤å…ï¼‰", value="2406 19th Ave, San Francisco, CA 94116", key="report_address_input")

    if st.button("æœç´¢é™„è¿‘é¤å…", type="primary", disabled=not google_key, key="btn_search_nearby"):
        geo = google_geocode(address_input, google_key)
        if not geo:
            st.error("æ— æ³•è§£æåœ°å€ï¼Œè¯·è¾“å…¥æ›´å®Œæ•´åœ°å€ï¼ˆå«åŸå¸‚/å·ï¼‰ã€‚")
        else:
            lat, lng = geo
            places = google_nearby_restaurants(lat, lng, google_key, radius_m=nearby_radius_m)
            st.session_state["geo"] = (lat, lng)
            st.session_state["places"] = places
            st.success(f"å·²æ‰¾åˆ° {len(places)} å®¶é™„è¿‘é¤å…ã€‚")

    places = st.session_state.get("places", [])
    place_details = st.session_state.get("place_details", {})

    if places:
        options, id_map = [], {}
        for p in places:
            name = p.get("name", "")
            addr = p.get("vicinity", "")
            rating = p.get("rating", "NA")
            total = p.get("user_ratings_total", "NA")
            pid = p.get("place_id", "")
            label = f"{name} | {addr} | â­{rating} ({total})"
            options.append(label)
            id_map[label] = pid

        selected_label = st.selectbox("é€‰æ‹©ç›®æ ‡é¤å…ï¼ˆGoogle Nearbyï¼‰", options, key="report_selected_label")
        selected_place_id = id_map.get(selected_label)

        if st.button("æ‹‰å–é¤å…è¯¦æƒ…ï¼ˆGoogle Place Detailsï¼‰", disabled=not google_key, key="btn_place_details"):
            if not selected_place_id:
                st.error("è¯·å…ˆé€‰æ‹©ä¸€ä¸ªé¤å…ã€‚")
            else:
                details = google_place_details(selected_place_id, google_key)
                if not details:
                    st.error("æ‹‰å–è¯¦æƒ…å¤±è´¥ã€‚")
                else:
                    st.session_state["place_details"] = details
                    st.success("å·²æ‹‰å–é¤å…è¯¦æƒ…ã€‚")
                    place_details = details


    # =========================================================
    # Step 2
    # =========================================================
    if place_details:
        st.subheader("Step 2ï½œä¸Šä¼ èœå• + è‡ªåŠ¨å•†åœˆç”»åƒï¼ˆACSï¼‰ + ç«å¯¹ï¼ˆGoogle/Yelp + ç«å¯¹èœå•ä¸Šä¼ ï¼‰")

        loc = (place_details.get("geometry", {}) or {}).get("location", {}) or {}
        rest_lat = float(loc.get("lat")) if loc.get("lat") is not None else None
        rest_lng = float(loc.get("lng")) if loc.get("lng") is not None else None

        col1, col2 = st.columns([1, 1])
        with col1:
            restaurant_en = st.text_input("é¤å…è‹±æ–‡å", value=place_details.get("name", ""), key="report_restaurant_en")
            restaurant_cn = st.text_input("é¤å…ä¸­æ–‡åï¼ˆå¯é€‰ï¼‰", value="", key="report_restaurant_cn")
            formatted_address = st.text_input("é¤å…åœ°å€", value=place_details.get("formatted_address", address_input), key="report_formatted_address")
            st.caption(f"Googleï¼šâ­{place_details.get('rating','')}ï¼ˆ{place_details.get('user_ratings_total','')} reviewsï¼‰")
            extra_context = st.text_area(
                "è¡¥å……ä¸šåŠ¡èƒŒæ™¯ï¼ˆå¯é€‰ï¼‰",
                value="ä¾‹å¦‚ï¼šç»è¥å¹´é™ã€ä¸»æ‰“èœã€ç›®æ ‡å®¢ç¾¤ã€å½“å‰ç—›ç‚¹ï¼ˆå•é‡/è¯„åˆ†/åˆ©æ¶¦/äººæ‰‹ç­‰ï¼‰ã€‚",
                height=120,
                key="report_extra_context"
            )
        with col2:
            st.markdown("### é—¨åº—å¤–å–èœå•ä¸Šä¼ ï¼ˆæ›¿ä»£å¹³å°é“¾æ¥ï¼‰")
            own_menu_files = st.file_uploader(
                "ä¸Šä¼ é—¨åº—èœå•ï¼ˆpng/jpg/txt/csv/xlsxï¼Œæ”¯æŒå¤šæ–‡ä»¶ï¼‰",
                type=["png", "jpg", "jpeg", "webp", "txt", "csv", "xlsx", "xls"],
                accept_multiple_files=True,
                key="own_menu_files"
            )
            st.caption("ç³»ç»Ÿä¼šè¯†åˆ«ï¼šèœå“ã€ä»·æ ¼ã€åˆ†ç±»ã€å¥—é¤ç»“æ„ã€ä¿ƒé”€æ–‡æ¡ˆï¼›æœ€ç»ˆå†™å…¥ PDFï¼ˆAppendix A + å›¾è¡¨é¡µï¼‰ã€‚")

        # ACS
        with st.expander("è‡ªåŠ¨è·å–å•†åœˆäººå£/æ”¶å…¥/å¹´é¾„/æ—è£”/ç§Ÿä½æ¯”ä¾‹ï¼ˆUS Census ACSï¼‰", expanded=True):
            if rest_lat and rest_lng:
                if st.button("è·å– ACS å•†åœˆç”»åƒï¼ˆè‡ªåŠ¨ï¼‰", key="btn_get_acs"):
                    tract_info = census_tract_from_latlng(rest_lat, rest_lng)
                    if not tract_info:
                        st.warning("æ— æ³•è·å– tract ä¿¡æ¯ï¼ˆCensus geocoderï¼‰ã€‚")
                    else:
                        acs_data = acs_5y_profile(tract_info["STATE"], tract_info["COUNTY"], tract_info["TRACT"], year=2023)
                        st.session_state["tract_info"] = tract_info
                        st.session_state["acs_data"] = acs_data
                        st.success("å·²è·å– ACS æ•°æ®ï¼ˆtract çº§åˆ«ä»£ç†ï¼‰ã€‚")
            else:
                st.info("æœªèƒ½ä» Google Place Details è·å–åæ ‡ï¼Œæ— æ³•è°ƒç”¨ ACSã€‚")

            tract_info = st.session_state.get("tract_info", None)
            acs_data = st.session_state.get("acs_data", None)
            if acs_data:
                st.write({
                    "ACS Year": acs_data.get("year"),
                    "Geography": acs_data.get("name"),
                    "Population (tract)": None if acs_data.get("pop_total") is None else int(acs_data.get("pop_total")),
                    "Median HH Income": None if acs_data.get("median_income") is None else f"${int(acs_data.get('median_income')):,}",
                    "Median Age": acs_data.get("median_age"),
                    "% Asian (proxy)": None if acs_data.get("pct_asian") is None else f"{acs_data.get('pct_asian')*100:.1f}%",
                    "% Renter (proxy)": None if acs_data.get("pct_renter") is None else f"{acs_data.get('pct_renter')*100:.1f}%",
                    "Note": "ACS ä¸º tract çº§åˆ«ä»£ç†ï¼Œä½œä¸ºå•†åœˆç”»åƒæ–¹å‘æ€§å‚è€ƒã€‚"
                })

        # Competitors
        st.markdown("### ç«å¯¹ä¿¡æ¯ï¼ˆå¯å¢åˆ è¡Œï¼šç”¨äºå·®å¼‚åŒ–ä¸ç«å“åˆ†æï¼‰")
        if "comp_rows" not in st.session_state:
            st.session_state.comp_rows = 3

        cA, cB, cC = st.columns([1, 1, 2])
        with cA:
            if st.button("â• æ·»åŠ ç«å¯¹", key="btn_add_comp"):
                st.session_state.comp_rows += 1
        with cB:
            if st.button("â– åˆ é™¤æœ€åä¸€ä¸ª", disabled=st.session_state.comp_rows <= 1, key="btn_del_comp"):
                st.session_state.comp_rows = max(1, st.session_state.comp_rows - 1)
        with cC:
            st.caption("æ¯ä¸ªç«å¯¹ï¼šå¡«å†™åç§°/åœ°å€ â†’ æ‹‰å– Google/Yelp â†’ ä¸Šä¼ ç«å¯¹èœå•æ–‡ä»¶ã€‚")

        comp_inputs: List[CompetitorInput] = []
        comp_summary_rows = []

        for i in range(st.session_state.comp_rows):
            with st.container(border=True):
                st.markdown(f"ç«å¯¹ #{i+1}")
                cc1, cc2, cc3 = st.columns([2, 2, 2])
                with cc1:
                    comp_name = st.text_input(f"ç«å¯¹åç§°æˆ–åœ°å€ï¼ˆ#{i+1}ï¼‰", value="", key=f"comp_name_{i}")
                with cc2:
                    comp_notes = st.text_input(f"å¤‡æ³¨ï¼ˆå¯é€‰ #{i+1}ï¼‰", value="", key=f"comp_notes_{i}")
                with cc3:
                    comp_menu_files = st.file_uploader(
                        f"ä¸Šä¼ ç«å¯¹èœå•ï¼ˆ#{i+1}ï¼‰png/jpg/txt/csv/xlsx",
                        type=["png", "jpg", "jpeg", "webp", "txt", "csv", "xlsx", "xls"],
                        accept_multiple_files=True,
                        key=f"comp_menu_files_{i}"
                    )

                pull_col1, pull_col2 = st.columns([1, 2])
                with pull_col1:
                    pull = st.button(f"æ‹‰å–ç«å¯¹ Google + Yelpï¼ˆ#{i+1}ï¼‰", key=f"pull_comp_{i}", disabled=not google_key)
                with pull_col2:
                    st.caption("Google: è¯„åˆ†/è¯„è®º/ä½ç½®/è¥ä¸šæ—¶é—´ï¼›Yelp: ä»·æ ¼å¸¦/åˆ†ç±»/è¯„è®ºç¤ºä¾‹ï¼ˆå¦‚é…ç½®äº†YELP_API_KEYï¼‰ã€‚")

                comp_google = st.session_state.get(f"comp_google_{i}", {})
                comp_yelp = st.session_state.get(f"comp_yelp_{i}", {})

                if pull and comp_name.strip():
                    pid = google_textsearch_place_id(f"{comp_name} {formatted_address}", google_key)
                    comp_google = google_place_details(pid, google_key) if pid else {"error": "Google textsearch failed"}
                    st.session_state[f"comp_google_{i}"] = comp_google

                    if yelp_key:
                        ysr = yelp_search_business(comp_name, formatted_address, yelp_key, limit=3)
                        best = None
                        if isinstance(ysr, dict) and ysr.get("businesses"):
                            best = ysr["businesses"][0]
                            rid = best.get("id")
                            rev = yelp_get_reviews(rid, yelp_key) if rid else {}
                            comp_yelp = {"best_match": best, "reviews": rev}
                        else:
                            comp_yelp = {"error": "No Yelp match", "raw": ysr}
                    else:
                        comp_yelp = {"note": "YELP_API_KEY not configured"}
                    st.session_state[f"comp_yelp_{i}"] = comp_yelp

                comp_summary_rows.append({
                    "competitor": comp_name.strip(),
                    "google_rating": comp_google.get("rating", "") if isinstance(comp_google, dict) else "",
                    "google_reviews": comp_google.get("user_ratings_total", "") if isinstance(comp_google, dict) else "",
                    "yelp_rating": (comp_yelp.get("best_match", {}) or {}).get("rating", "") if isinstance(comp_yelp, dict) else "",
                    "yelp_reviews": (comp_yelp.get("best_match", {}) or {}).get("review_count", "") if isinstance(comp_yelp, dict) else "",
                    "menus_uploaded": 0 if not comp_menu_files else len(comp_menu_files),
                })

                comp_inputs.append(
                    CompetitorInput(
                        name_or_address=comp_name.strip(),
                        notes=comp_notes.strip(),
                        menu_files_meta={"label": f"COMP_{i+1}", "files": [{"name": f.name} for f in (comp_menu_files or [])],
                                         "extracted": {"items": [], "promos": [], "notes": []}},
                        google=comp_google if isinstance(comp_google, dict) else {},
                        yelp=comp_yelp if isinstance(comp_yelp, dict) else {},
                    )
                )

        if comp_summary_rows:
            st.dataframe(pd.DataFrame(comp_summary_rows), use_container_width=True)

        # Orders
        with st.expander("ä¸Šä¼ è®¢å•æŠ¥è¡¨ï¼ˆCSVï¼Œå¯é€‰ï¼šç”¨äºæ—¶æ®µ/å®¢å•/çƒ­é”€/KPIï¼‰", expanded=False):
            order_files = st.file_uploader("ä¸Šä¼ å¹³å°è®¢å•å¯¼å‡º CSVï¼ˆå¯å¤šé€‰ï¼‰", type=["csv"], accept_multiple_files=True, key="report_order_files")
            orders_meta = summarize_orders(order_files or [])
            if order_files:
                st.json(orders_meta)

        if "orders_meta" not in locals():
            orders_meta = {"files": [], "note": "No uploads"}

        # Step 3
        st.subheader("Step 3ï½œç”Ÿæˆæ·±åº¦åˆ†ææŠ¥å‘Šï¼ˆå«å¯è§†åŒ–å›¾è¡¨ + å¯æ‰§è¡ŒåŠ¨ä½œå±•å¼€ï¼‰")

        if st.button("ç”ŸæˆæŠ¥å‘Šå†…å®¹", type="primary", disabled=not openai_key, key="btn_generate_report"):
            progress = st.progress(0)
            status = st.empty()

            def step(pct: int, msg: str):
                progress.progress(pct)
                status.info(msg)

            report_date = dt.datetime.now().strftime("%m/%d/%Y")

            step(5, "æ­£åœ¨è§£æé—¨åº—èœå•ï¼ˆè¯†åˆ«èœå“/ä»·æ ¼/ä¿ƒé”€ï¼‰...")
            own_menu_meta = extract_menu_with_openai(own_menu_files or [], openai_key, model, label="OWN_MENU")

            step(25, "æ­£åœ¨è§£æç«å¯¹èœå•ï¼ˆé€ä¸ªè¯†åˆ«èœå“/ä»·æ ¼/ä¿ƒé”€ï¼‰...")
            competitors_full: List[CompetitorInput] = []
            for i in range(st.session_state.comp_rows):
                comp_name = st.session_state.get(f"comp_name_{i}", "").strip()
                comp_notes = st.session_state.get(f"comp_notes_{i}", "").strip()
                comp_google = st.session_state.get(f"comp_google_{i}", {}) or {}
                comp_yelp = st.session_state.get(f"comp_yelp_{i}", {}) or {}
                comp_files = st.session_state.get(f"comp_menu_files_{i}", None)

                if comp_files and isinstance(comp_files, list) and len(comp_files) > 0:
                    comp_menu_meta = extract_menu_with_openai(comp_files, openai_key, model, label=f"COMP_{i+1}")
                else:
                    comp_menu_meta = {"label": f"COMP_{i+1}", "files": [], "extracted": {"items": [], "promos": [], "notes": ["no competitor menu uploaded"]}}

                competitors_full.append(CompetitorInput(
                    name_or_address=comp_name,
                    notes=comp_notes,
                    menu_files_meta=comp_menu_meta,
                    google=comp_google,
                    yelp=comp_yelp
                ))

            step(45, "æ­£åœ¨ç”Ÿæˆèœå•å¯è§†åŒ–å›¾è¡¨ï¼ˆä»·æ ¼åˆ†å¸ƒ/å“ç±»ç»“æ„/ä»·æ ¼å¸¦/ç«å¯¹å¯¹æ¯”ï¼‰...")
            own_df = menu_to_df(own_menu_meta)
            comp_dfs = []
            for c in competitors_full:
                dfc = menu_to_df(c.menu_files_meta)
                comp_dfs.append((c.name_or_address or c.menu_files_meta.get("label", "Competitor"), dfc))
            charts = build_charts(own_df, comp_dfs)

            step(60, "æ­£åœ¨ç”Ÿæˆå’¨è¯¢çº§æŠ¥å‘Šï¼ˆå«ï¼šä¹°ä¸€é€ä¸€å…·ä½“èœå“ã€å­£èŠ‚æ€§ä¸Šæ–°ã€åŠ¨ä½œæ¸…å•å±•å¼€ï¼‰...")
            tract_info = st.session_state.get("tract_info", None)
            acs_data = st.session_state.get("acs_data", None)

            inputs = ReportInputs(
                report_date=report_date,
                restaurant_cn=(restaurant_cn.strip() or restaurant_en.strip()),
                restaurant_en=restaurant_en.strip(),
                address=formatted_address.strip(),
                radius_miles=radius_miles,
                own_menu_meta=own_menu_meta,
                orders_meta=orders_meta,
                competitors=competitors_full,
                extra_business_context=extra_context.strip(),
                acs=acs_data,
                tract_info=tract_info,
                restaurant_google=place_details,
                charts=charts,
            )

            prompt = build_prompt(inputs)

            report_text = openai_text(prompt, openai_key, model=model, temperature=0.25)
            report_text = sanitize_text(report_text)

            step(80, "æ­£åœ¨è‡ªåŠ¨æ‰©å†™ï¼ˆç¡®ä¿è¶³å¤Ÿé•¿ã€è¶³å¤Ÿç»†ã€èƒ½è½åœ°æ‰§è¡Œï¼‰...")
            report_text = ensure_long_enough(report_text, openai_key, model=model, min_chars=16000)

            step(95, "æ­£åœ¨å®Œæˆæ¸²æŸ“å‡†å¤‡ï¼ˆå¯ç¼–è¾‘é¢„è§ˆ + PDFï¼‰...")
            st.session_state["report_text"] = report_text
            st.session_state["report_inputs"] = inputs

            step(100, "å®Œæˆï¼šæŠ¥å‘Šå†…å®¹å·²ç”Ÿæˆã€‚ä½ å¯ä»¥é¢„è§ˆç¼–è¾‘å¹¶è¾“å‡º PDFã€‚")
            status.success("æŠ¥å‘Šå†…å®¹å·²ç”Ÿæˆï¼ˆåŒ…å«å›¾è¡¨é¡µä¸æ›´å®Œæ•´çš„ç¬¬6ç« å®šä»·/ä¿ƒé”€æ·±åº¦åˆ†æï¼‰ã€‚")

    # Preview + PDF
    report_text = st.session_state.get("report_text", "")
    report_inputs: Optional[ReportInputs] = st.session_state.get("report_inputs", None)

    if report_text and report_inputs:
        st.subheader("é¢„è§ˆï¼ˆå¯ç¼–è¾‘ï¼‰")
        edited = st.text_area("æŠ¥å‘Šæ­£æ–‡ï¼ˆä½ å¯ä»¥ç›´æ¥ä¿®æ”¹ï¼‰", value=report_text, height=520, key="report_editor")
        st.session_state["report_text"] = sanitize_text(edited)

        st.subheader("å›¾è¡¨é¢„è§ˆï¼ˆå°†è‡ªåŠ¨é™„åœ¨ PDF åé¢ï¼‰")
        if report_inputs.charts:
            for k, v in report_inputs.charts.items():
                st.markdown(f"**{k}**")
                st.image(v, use_container_width=True)
        else:
            st.info("æš‚æ— å›¾è¡¨ï¼ˆé€šå¸¸æ˜¯èœå•ä»·æ ¼è¯†åˆ«ä¸è¶³å¯¼è‡´ price ç¼ºå¤±ï¼‰ã€‚")

        st.subheader("Step 4ï½œç”Ÿæˆ PDFï¼ˆå«å›¾è¡¨é¡µï¼‰")
        if st.button("ç”Ÿæˆ PDF", type="primary", key="btn_make_pdf"):
            with st.spinner("æ­£åœ¨ç”Ÿæˆ PDF..."):
                pdf_path = render_pdf(st.session_state["report_text"], report_inputs)
            st.success("PDF ç”Ÿæˆå®Œæˆã€‚")
            with open(pdf_path, "rb") as f:
                st.download_button("ä¸‹è½½ PDF", f, file_name=os.path.basename(pdf_path), mime="application/pdf", key="btn_dl_pdf")
            st.caption(f"è¾“å‡ºè·¯å¾„ï¼š{pdf_path}")
    else:
        st.info("å®Œæˆé¤å…é€‰æ‹© â†’ ä¸Šä¼ èœå•/ç«å¯¹ â†’ ç”ŸæˆæŠ¥å‘Šåï¼Œè¿™é‡Œä¼šæ˜¾ç¤ºé¢„è§ˆä¸ PDF ä¸‹è½½ã€‚")


# =========================================================
# TAB 2: MENU SMART ADJUST (NEW)
# =========================================================
with tab_menu:
    st.subheader("èœå•æ™ºèƒ½è°ƒæ•´ï¼ˆæŒ‰ä½ è¦æ±‚çš„ Step Cï¼‰")
    st.caption("é€»è¾‘ï¼šä¸Šä¼ èœå• â†’ AIè¯†åˆ« â†’ åˆ¤æ–­â€œå¥—é¤ä¸è¶³/æ•´ä½“åè´µâ€ â†’ å…ˆè°ƒä»· â†’ å†ç»„å¥—é¤ â†’ è¾“å‡ºå®Œæ•´èœå•ç»“æ„ï¼ˆå¯ä¸‹è½½CSV/JSONï¼‰ã€‚")

    st.divider()

    colL, colR = st.columns([2, 1], gap="large")
    with colR:
        st.markdown("### å‚æ•°")
        location = st.text_input("é—¨åº—/å•†åœˆä½ç½®ï¼ˆç”¨äºåŒè¡Œä»·å¸¦ä¼°ç®—ï¼‰", value="San Francisco Bay Area", key="menu_location")
        cuisine = st.text_input("èœç³»/å®šä½ï¼ˆç”¨äºåŒè¡Œä»·å¸¦ä¼°ç®—ï¼‰", value="æ¸¯å¼/ä¸œå—äºš/ä¸­é¤å¤–å–", key="menu_cuisine")
        target_median = st.number_input("ç›®æ ‡ä¸»é£Ÿä¸­ä½ä»·ï¼ˆåé«˜åˆ¤å®šåŸºå‡†ï¼‰", value=float(DEFAULT_TARGET_MEDIAN_MAIN), step=0.5, key="menu_target_median")
        high_ratio = st.number_input("åé«˜é˜ˆå€¼å€ç‡ï¼ˆå½“å‰ä¸­ä½æ•° > ç›®æ ‡*å€ç‡ï¼‰", value=float(HIGH_PRICE_THRESHOLD_RATIO), step=0.01, key="menu_high_ratio")
        min_combos = st.number_input("æœ€ä½å¥—é¤æ•°é‡ï¼ˆä¸è¶³åˆ™è‡ªåŠ¨ç»„å»ºï¼‰", value=int(MIN_COMBOS_REQUIRED), step=1, key="menu_min_combos")
        front_limit = st.number_input("å‰å°å»ºè®® SKU ä¸Šé™ï¼ˆå‚è€ƒï¼‰", value=int(FRONT_LIMIT), step=1, key="menu_front_limit")
        st.caption("è¯´æ˜ï¼šä½ æ²¡ä¸Šä¼ ç«å“èœå•ä¹Ÿèƒ½è·‘ã€‚åŒè¡Œä»·å¸¦ç”± AI ä¼°ç®—ï¼›è‹¥ AI å¤±è´¥ï¼Œè‡³å°‘èƒ½æŒ‰ç›®æ ‡ä¸­ä½æ•°åšè§„åˆ™å›è°ƒã€‚")

    with colL:
        st.markdown("### ä¸Šä¼ èœå•ï¼ˆå›¾ç‰‡/CSV/Excelï¼‰")
        menu_files = st.file_uploader(
            "ä¸Šä¼ èœå•æ–‡ä»¶ï¼ˆpng/jpg/txt/csv/xlsxï¼Œæ”¯æŒå¤šæ–‡ä»¶ï¼‰",
            type=["png", "jpg", "jpeg", "webp", "txt", "csv", "xlsx", "xls"],
            accept_multiple_files=True,
            key="smart_menu_files"
        )
        st.caption("å»ºè®®ï¼šä¸Šä¼ â€œå¹³å°èœå•é¡µæˆªå›¾â€æ¯”â€œåå°åˆ—è¡¨æˆªå›¾â€æ›´å¥½è¯†åˆ«ã€‚åå°åˆ—è¡¨æˆªå›¾ä¹Ÿèƒ½ç”¨ï¼Œä½†ç¼ºå£ä¼šå¤šä¸€äº›ã€‚")

        run_smart = st.button("ğŸš€ å¼€å§‹æ™ºèƒ½è°ƒæ•´å¹¶ç”Ÿæˆå®Œæ•´èœå•", type="primary", disabled=not openai_key, key="btn_run_smart_menu")

    if run_smart:
        prog = st.progress(0)
        stat = st.empty()

        def step(p, msg):
            prog.progress(p)
            stat.info(msg)

        # Step A: Extract
        step(10, "æ­£åœ¨è§£æèœå•ï¼ˆè¯†åˆ«èœå“/ä»·æ ¼/åˆ†ç±»/å¥—é¤çº¿ç´¢ï¼‰...")
        menu_meta = extract_menu_with_openai(menu_files or [], openai_key, model, label="SMART_MENU")
        items = normalize_extracted_items(menu_meta)

        if not items:
            st.error("æœªè¯†åˆ«åˆ°ä»»ä½•èœå“ã€‚å»ºè®®æ¢æ›´æ¸…æ™°çš„èœå•å›¾ç‰‡ï¼Œæˆ–ä¸Šä¼  CSV/Excelã€‚")
            st.stop()

        typed0 = split_by_type(items)
        mains0 = [m for m in typed0["main"] if isinstance(m.get("price"), (int, float)) and m["price"] and m["price"] > 0]
        combos0 = typed0["combo"]

        step(25, "Step Cï¼šæ£€æŸ¥å¥—é¤ & æ£€æŸ¥æ•´ä½“ä»·æ ¼æ˜¯å¦åé«˜ï¼ˆåŒè¡Œä»·å¸¦ä¼°ç®—ï¼‰...")

        # Step C-1: overpriced check (rule)
        overpriced, cur_med = is_menu_overpriced(mains0, float(target_median), float(high_ratio)) if mains0 else (False, None)

        meta = {
            "current_median_main": cur_med,
            "target_median_main": float(target_median),
            "overpriced_by_rule": overpriced,
            "combo_count_detected": len(combos0),
        }

        # Step C-2: If overpriced -> first adjust prices (AI first, fallback rule)
        adjusted_items = items
        ai_meta = {}
        if overpriced:
            step(40, "æ£€æµ‹åˆ°æ•´ä½“åè´µï¼šå…ˆè¿›è¡Œ AI åŒè¡Œä»·å¸¦æ ¡å‡†ï¼ˆå¤±è´¥åˆ™è§„åˆ™ç¼©æ”¾å›è°ƒï¼‰...")
            adjusted_items_ai, ai_meta = ai_market_benchmark(
                items=items,
                location=location,
                cuisine=cuisine,
                target_median_main=float(target_median),
                api_key=openai_key,
                model=model
            )

            # If AI provides usable adjustment, apply it; else fallback to rule scaling
            if ai_meta and not ai_meta.get("ai_error"):
                adjusted_items = adjusted_items_ai
                meta["price_adjustment_mode"] = "ai_market_benchmark"
                meta["ai_market_meta"] = ai_meta
            else:
                # fallback: scale to target median
                if cur_med and cur_med > 0:
                    scale = float(target_median) / float(cur_med)
                    adjusted_items = scale_prices(items, scale=scale)
                    meta["price_adjustment_mode"] = "rule_scale"
                    meta["rule_scale"] = round(scale, 4)
                meta["ai_market_meta"] = ai_meta

        # Step C-3: If combo missing -> build combos AFTER pricing adjusted
        typed1 = split_by_type(adjusted_items)
        combos1 = typed1["combo"]
        combo_ok = detect_combo_system(combos1, int(min_combos))

        if not combo_ok:
            step(65, f"å¥—é¤ä¸è¶³ï¼ˆå½“å‰ {len(combos1)} < {int(min_combos)}ï¼‰ï¼šå…ˆè°ƒä»·å·²å®Œæˆ â†’ ç°åœ¨è‡ªåŠ¨ç»„å»ºå¥—é¤...")
            adjusted_items, combo_meta = build_combos_if_missing(
                items=adjusted_items,
                location=location,
                cuisine=cuisine,
                api_key=openai_key,
                model=model
            )
            meta["combo_build_meta"] = combo_meta
        else:
            meta["combo_build_meta"] = {"note": "combos sufficient"}

        # Step Final: Build final menu
        step(85, "æ­£åœ¨ç”Ÿæˆå®Œæ•´èœå•ç»“æ„ï¼ˆé”šç‚¹/çˆ†å“/å¥—é¤/åŠ è´­/é¥®å“/éšè—å±‚ï¼‰...")
        final_menu = build_smart_menu(adjusted_items, front_limit=int(front_limit))
        flat_df = menu_to_flat_df(final_menu)

        step(100, "å®Œæˆï¼šå·²ç”Ÿæˆå®Œæ•´èœå•ç»“æ„ã€‚")

        st.success("âœ… èœå•æ™ºèƒ½è°ƒæ•´å®Œæˆï¼ˆæŒ‰ Step Cï¼šå…ˆè°ƒä»· â†’ å†ç»„å¥—é¤ â†’ è¾“å‡ºå®Œæ•´èœå•ï¼‰")
        st.write(meta)

        st.divider()
        st.markdown("## æœ€ç»ˆèœå•ç»“æ„é¢„è§ˆ")
        for section, lst in final_menu.items():
            st.markdown(f"### {section}ï¼ˆ{len(lst)}ï¼‰")
            for it in lst[:80]:
                nm = item_fullname(it)
                p = it.get("price")
                po = it.get("price_old", None)
                if isinstance(po, (int, float)) and isinstance(p, (int, float)) and po != p:
                    st.write(f"- {nm} â€” ${p:.2f}ï¼ˆåŸä»· ${po:.2f}ï¼‰")
                elif isinstance(p, (int, float)):
                    st.write(f"- {nm} â€” ${p:.2f}")
                else:
                    st.write(f"- {nm} â€” ä»·æ ¼ç¼ºå¤±")

            if len(lst) > 80:
                st.caption(f"ï¼ˆæ­¤å¤„ä»…é¢„è§ˆå‰ 80 é¡¹ï¼Œå®Œæ•´è¯·ä¸‹è½½ CSV/JSONï¼‰")

        st.divider()
        st.markdown("## ä¸‹è½½")
        menu_json = json.dumps(final_menu, ensure_ascii=False, indent=2)
        st.download_button("ä¸‹è½½èœå•ç»“æ„ JSON", data=menu_json, file_name="smart_menu.json", mime="application/json", key="dl_smart_json")

        st.dataframe(flat_df, use_container_width=True, height=520)
        st.download_button("ä¸‹è½½èœå•ç»“æ„ CSV", data=flat_df.to_csv(index=False), file_name="smart_menu.csv", mime="text/csv", key="dl_smart_csv")
