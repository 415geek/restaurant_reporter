import os
import re
import json
import time
import math
import textwrap
import datetime as dt
from dataclasses import dataclass
from typing import List, Dict, Any, Optional, Tuple

import requests
import streamlit as st
import pandas as pd

from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas
from reportlab.lib.units import inch
from reportlab.lib import colors
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
import hmac


# =========================================================
# Streamlit Page Config (ONLY ONCE)
# =========================================================
APP_TITLE = "AuraInsight æŠ¥å‘Šç”Ÿæˆå™¨ï¼ˆTrade Area & Growth Diagnosticï¼‰"
st.set_page_config(page_title=APP_TITLE, layout="wide")


# =========================================================
# Login Gate
# =========================================================
def _get_allowed_passwords():
    pw_list = st.secrets.get("ADMIN_PASSWORDS", None)
    if pw_list and isinstance(pw_list, (list, tuple)) and len(pw_list) > 0:
        return [str(x) for x in pw_list]
    single = st.secrets.get("ADMIN_PASSWORD", "")
    return [str(single)] if single else []

def _secure_compare(a: str, b: str) -> bool:
    return hmac.compare_digest(a.encode("utf-8"), b.encode("utf-8"))

def require_login():
    if "auth_ok" not in st.session_state:
        st.session_state.auth_ok = False
    if "auth_tries" not in st.session_state:
        st.session_state.auth_tries = 0
    if "auth_locked_until" not in st.session_state:
        st.session_state.auth_locked_until = 0.0

    if st.session_state.auth_ok:
        return True

    now = time.time()
    if now < st.session_state.auth_locked_until:
        wait_s = int(st.session_state.auth_locked_until - now)
        st.error(f"å°è¯•æ¬¡æ•°è¿‡å¤šï¼Œè¯· {wait_s}s åå†è¯•ã€‚")
        st.stop()

    allowed = _get_allowed_passwords()
    if not allowed:
        st.error("æœªé…ç½®ç®¡ç†å‘˜å¯†ç ï¼šè¯·åœ¨ .streamlit/secrets.toml è®¾ç½® ADMIN_PASSWORD æˆ– ADMIN_PASSWORDSã€‚")
        st.stop()

    st.markdown("## ğŸ”’ AuraInsight ç®¡ç†å‘˜ç™»å½•")
    st.caption("è¯·è¾“å…¥ç®¡ç†å‘˜è®¾ç½®çš„å¯†ç åè¿›å…¥å·¥å…·ã€‚")

    pw = st.text_input("å¯†ç ", type="password")
    col1, col2 = st.columns([1, 1])

    with col1:
        if st.button("ç™»å½•", type="primary"):
            ok = any(_secure_compare(pw, x) for x in allowed)
            if ok:
                st.session_state.auth_ok = True
                st.session_state.auth_tries = 0
                st.success("ç™»å½•æˆåŠŸã€‚")
                st.rerun()
            else:
                st.session_state.auth_tries += 1
                st.error("å¯†ç é”™è¯¯ã€‚")
                if st.session_state.auth_tries >= 5:
                    st.session_state.auth_locked_until = time.time() + 60
                    st.session_state.auth_tries = 0
                    st.warning("å·²æš‚æ—¶é”å®š 60 ç§’ã€‚")

    with col2:
        if st.button("æ¸…ç©º"):
            st.rerun()

    st.stop()

def logout_button():
    if st.button("ç™»å‡º"):
        st.session_state.auth_ok = False
        st.session_state.auth_locked_until = 0.0
        st.session_state.auth_tries = 0
        st.rerun()


# =========================================================
# MUST require login before tool UI
# =========================================================
require_login()


# =========================================================
# Config (fixed / hidden from sidebar)
# =========================================================
OUTPUT_DIR = "output"
ASSETS_DIR = "assets"
FONTS_DIR = os.path.join(ASSETS_DIR, "fonts")

BG_COVER = os.path.join(ASSETS_DIR, "bg_cover.png")
BG_CONTENT = os.path.join(ASSETS_DIR, "bg_content.png")

# IMPORTANT:
# Your cover/content background already has the big title.
# So we DO NOT draw AuraInsight/ã€é—¨åº—åˆ†ææŠ¥å‘Šã€‘ again.
COVER_HAS_BRANDING = True
CONTENT_BG_HAS_HEADER_BARS = True  # your content background has the top bars

# Fonts (static ttf)
FONT_NOTO_REG = os.path.join(FONTS_DIR, "NotoSansSC-Regular.ttf")
FONT_NOTO_BOLD = os.path.join(FONTS_DIR, "NotoSansSC-Bold.ttf")
FONT_ROBOTO_REG = os.path.join(FONTS_DIR, "Roboto-Regular.ttf")
FONT_ROBOTO_BOLD = os.path.join(FONTS_DIR, "Roboto-Bold.ttf")
FONT_ROBOTO_ITALIC = os.path.join(FONTS_DIR, "Roboto-Italic.ttf")

PAGE_W, PAGE_H = letter  # 612 x 792


# =========================================================
# Data Models
# =========================================================
@dataclass
class ReportInputs:
    report_date: str
    restaurant_cn: str
    restaurant_en: str
    address: str
    radius_miles: float
    platform_links: Dict[str, str]
    competitors: List[Dict[str, str]]
    competitor_menu_snapshot: str
    your_menu_snapshot: str
    order_upload_meta: Dict[str, Any]
    extra_business_context: str


# =========================================================
# Fonts / Typography
# =========================================================
def register_aurainsight_fonts():
    if os.path.exists(FONT_NOTO_REG):
        pdfmetrics.registerFont(TTFont("Noto", FONT_NOTO_REG))
    if os.path.exists(FONT_NOTO_BOLD):
        pdfmetrics.registerFont(TTFont("Noto-Bold", FONT_NOTO_BOLD))
    if os.path.exists(FONT_ROBOTO_REG):
        pdfmetrics.registerFont(TTFont("Roboto", FONT_ROBOTO_REG))
    if os.path.exists(FONT_ROBOTO_BOLD):
        pdfmetrics.registerFont(TTFont("Roboto-Bold", FONT_ROBOTO_BOLD))
    if os.path.exists(FONT_ROBOTO_ITALIC):
        pdfmetrics.registerFont(TTFont("Roboto-Italic", FONT_ROBOTO_ITALIC))

def f_cn(bold: bool = False) -> str:
    if bold and "Noto-Bold" in pdfmetrics.getRegisteredFontNames():
        return "Noto-Bold"
    if "Noto" in pdfmetrics.getRegisteredFontNames():
        return "Noto"
    return "Helvetica-Bold" if bold else "Helvetica"

def f_en(bold: bool = False, italic: bool = False) -> str:
    if italic and "Roboto-Italic" in pdfmetrics.getRegisteredFontNames():
        return "Roboto-Italic"
    if bold and "Roboto-Bold" in pdfmetrics.getRegisteredFontNames():
        return "Roboto-Bold"
    if "Roboto" in pdfmetrics.getRegisteredFontNames():
        return "Roboto"
    return "Helvetica-Bold" if bold else "Helvetica"

def is_ascii_line(s: str) -> bool:
    s = s.strip()
    return bool(s) and all(ord(ch) < 128 for ch in s)


# =========================================================
# Google Places
# =========================================================
def google_geocode(address: str, api_key: str) -> Optional[Tuple[float, float]]:
    url = "https://maps.googleapis.com/maps/api/geocode/json"
    r = requests.get(url, params={"address": address, "key": api_key}, timeout=30)
    r.raise_for_status()
    data = r.json()
    if data.get("status") != "OK" or not data.get("results"):
        return None
    loc = data["results"][0]["geometry"]["location"]
    return float(loc["lat"]), float(loc["lng"])

def google_nearby_restaurants(lat: float, lng: float, api_key: str, radius_m: int = 1200) -> List[Dict[str, Any]]:
    url = "https://maps.googleapis.com/maps/api/place/nearbysearch/json"
    results: List[Dict[str, Any]] = []
    params = {"location": f"{lat},{lng}", "radius": radius_m, "type": "restaurant", "key": api_key}

    for _ in range(3):
        r = requests.get(url, params=params, timeout=30)
        r.raise_for_status()
        data = r.json()
        if data.get("status") not in ("OK", "ZERO_RESULTS"):
            break
        results.extend(data.get("results", []))
        token = data.get("next_page_token")
        if not token:
            break
        time.sleep(2)
        params = {"pagetoken": token, "key": api_key}

    return results

def google_place_details(place_id: str, api_key: str) -> Dict[str, Any]:
    url = "https://maps.googleapis.com/maps/api/place/details/json"
    fields = ",".join([
        "name", "formatted_address", "rating", "user_ratings_total",
        "types", "url", "website", "formatted_phone_number",
        "opening_hours", "reviews", "geometry"
    ])
    r = requests.get(url, params={"place_id": place_id, "fields": fields, "key": api_key}, timeout=30)
    r.raise_for_status()
    data = r.json()
    if data.get("status") != "OK":
        return {}
    return data.get("result", {})

def google_textsearch_place_id(query: str, api_key: str) -> Optional[str]:
    url = "https://maps.googleapis.com/maps/api/place/textsearch/json"
    r = requests.get(url, params={"query": query, "key": api_key}, timeout=30)
    r.raise_for_status()
    data = r.json()
    if data.get("status") != "OK" or not data.get("results"):
        return None
    return data["results"][0].get("place_id")


# =========================================================
# Census ACS (Demographics)
# =========================================================
def census_tract_from_latlng(lat: float, lng: float) -> Optional[Dict[str, str]]:
    url = "https://geocoding.geo.census.gov/geocoder/geographies/coordinates"
    params = {
        "x": lng,
        "y": lat,
        "benchmark": "Public_AR_Current",
        "vintage": "Current_Current",
        "format": "json"
    }
    r = requests.get(url, params=params, timeout=30)
    r.raise_for_status()
    data = r.json()
    try:
        geos = data["result"]["geographies"]
        tract = geos["Census Tracts"][0]
        return {
            "STATE": tract["STATE"],
            "COUNTY": tract["COUNTY"],
            "TRACT": tract["TRACT"],
            "NAME": tract.get("NAME", "")
        }
    except Exception:
        return None

def acs_5y_profile(state: str, county: str, tract: str, year: int = 2023) -> Optional[Dict[str, Any]]:
    vars_map = {
        "pop_total": "B01003_001E",
        "median_income": "B19013_001E",
        "median_age": "B01002_001E",
        "white": "B02001_002E",
        "black": "B02001_003E",
        "asian": "B02001_005E",
        "other": "B02001_007E",
        "hispanic": "B03003_003E",
        "housing_units": "B25001_001E",
        "owner_occ": "B25003_002E",
        "renter_occ": "B25003_003E",
        "avg_hh_size": "B25010_001E",
    }
    get_vars = ",".join(["NAME"] + list(vars_map.values()))
    url = f"https://api.census.gov/data/{year}/acs/acs5"
    params = {"get": get_vars, "for": f"tract:{tract}", "in": f"state:{state} county:{county}"}
    r = requests.get(url, params=params, timeout=30)
    r.raise_for_status()
    data = r.json()
    if not data or len(data) < 2:
        return None

    headers = data[0]
    values = data[1]
    row = dict(zip(headers, values))

    def to_num(x):
        try:
            return float(x)
        except Exception:
            return None

    out = {"year": year, "name": row.get("NAME", ""), "state": state, "county": county, "tract": tract}
    for k, v in vars_map.items():
        out[k] = to_num(row.get(v))

    pop = out.get("pop_total") or 0

    def pct(x):
        if pop <= 0 or x is None:
            return None
        return x / pop

    out["pct_asian"] = pct(out.get("asian"))
    out["pct_white"] = pct(out.get("white"))
    out["pct_black"] = pct(out.get("black"))
    out["pct_hispanic"] = pct(out.get("hispanic"))

    owner = out.get("owner_occ")
    renter = out.get("renter_occ")
    occ_total = (owner or 0) + (renter or 0)
    if occ_total > 0:
        out["pct_owner"] = (owner or 0) / occ_total
        out["pct_renter"] = (renter or 0) / occ_total
    else:
        out["pct_owner"] = None
        out["pct_renter"] = None

    return out


# =========================================================
# OpenAI (Responses API)
# =========================================================
def openai_generate(prompt: str, api_key: str, model: str) -> str:
    url = "https://api.openai.com/v1/responses"
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    payload = {"model": model, "input": prompt, "temperature": 0.35}
    r = requests.post(url, headers=headers, json=payload, timeout=180)
    r.raise_for_status()
    data = r.json()

    out = []
    for item in data.get("output", []):
        for c in item.get("content", []):
            if c.get("type") == "output_text":
                out.append(c.get("text", ""))
    return "\n".join(out).strip()


# =========================================================
# Text Sanitization (Stop Markdown leakage)
# =========================================================
def sanitize_text(text: str) -> str:
    text = re.sub(r'^\s{0,3}#{1,6}\s*', '', text, flags=re.M)
    text = re.sub(r'\*\*(.*?)\*\*', r'\1', text)
    text = re.sub(r'^\s*\|?\s*:?-{3,}:?\s*(\|\s*:?-{3,}:?\s*)+\|?\s*$', '', text, flags=re.M)
    text = text.replace("```", "").replace("`", "")
    text = text.replace("â€¢", "-")
    return text.strip()


# =========================================================
# Upload parsing (menu snapshots)
# =========================================================
def read_uploaded_text(file) -> str:
    """
    Supports: .txt .csv .xlsx
    Return a compact textual snapshot for model.
    """
    name = getattr(file, "name", "uploaded")
    ext = os.path.splitext(name)[1].lower()

    try:
        if ext == ".txt":
            raw = file.read()
            if isinstance(raw, bytes):
                return raw.decode("utf-8", errors="ignore")
            return str(raw)

        if ext == ".csv":
            df = pd.read_csv(file)
            # Expect columns like: item/name, price, category, promo...
            # Convert first 200 rows into readable lines
            lines = []
            cols = [c for c in df.columns]
            head_cols = cols[:8]
            lines.append(f"[CSV:{name}] columns={head_cols}")
            for i, row in df.head(200).iterrows():
                parts = []
                for c in head_cols:
                    v = row.get(c, "")
                    v = "" if pd.isna(v) else str(v)
                    v = v.replace("\n", " ").strip()
                    if v:
                        parts.append(f"{c}={v}")
                if parts:
                    lines.append(" | ".join(parts))
            return "\n".join(lines)

        if ext in (".xlsx", ".xls"):
            df = pd.read_excel(file)
            lines = []
            cols = [c for c in df.columns]
            head_cols = cols[:8]
            lines.append(f"[XLSX:{name}] columns={head_cols}")
            for i, row in df.head(200).iterrows():
                parts = []
                for c in head_cols:
                    v = row.get(c, "")
                    v = "" if pd.isna(v) else str(v)
                    v = v.replace("\n", " ").strip()
                    if v:
                        parts.append(f"{c}={v}")
                if parts:
                    lines.append(" | ".join(parts))
            return "\n".join(lines)

        # fallback
        raw = file.read()
        if isinstance(raw, bytes):
            return raw.decode("utf-8", errors="ignore")
        return str(raw)
    except Exception as e:
        return f"[ParseError:{name}] {str(e)[:200]}"


def build_menu_snapshot_from_uploads(files: List[Any], max_chars: int = 15000) -> str:
    if not files:
        return ""
    chunks = []
    for f in files:
        chunks.append(read_uploaded_text(f))
    merged = "\n\n".join(chunks).strip()
    if len(merged) > max_chars:
        merged = merged[:max_chars] + "\n...[TRUNCATED]"
    return merged


# =========================================================
# Prompt Builder (Consulting-grade, no Markdown)
# =========================================================
def build_prompt(
    place: Dict[str, Any],
    inputs: ReportInputs,
    competitor_places: List[Dict[str, Any]],
    acs: Optional[Dict[str, Any]],
    min_pages_target: int = 6,
) -> str:
    def safe(d, k, default=None):
        if not isinstance(d, dict):
            return default
        return d.get(k, default)

    reviews = safe(place, "reviews", []) or []
    reviews_sample = []
    for rv in reviews[:12]:
        reviews_sample.append({
            "rating": rv.get("rating"),
            "time": rv.get("relative_time_description"),
            "text": (rv.get("text") or "")[:320]
        })

    comp_brief = []
    for cp in competitor_places[:10]:
        comp_brief.append({
            "name": safe(cp, "name", ""),
            "address": safe(cp, "formatted_address", ""),
            "rating": safe(cp, "rating", ""),
            "user_ratings_total": safe(cp, "user_ratings_total", ""),
            "types": safe(cp, "types", []),
        })

    data_blob = {
        "restaurant": {
            "name": safe(place, "name", ""),
            "address": safe(place, "formatted_address", inputs.address),
            "rating": safe(place, "rating", ""),
            "user_ratings_total": safe(place, "user_ratings_total", ""),
            "phone": safe(place, "formatted_phone_number", ""),
            "website": safe(place, "website", ""),
            "google_url": safe(place, "url", ""),
            "opening_hours": safe(place, "opening_hours", {}),
            "types": safe(place, "types", []),
            "reviews_sample": reviews_sample,
        },
        "trade_area": {
            "radius_miles": inputs.radius_miles,
            "city": "San Francisco",
            "approximation_note": "demographics use tract-level ACS near restaurant coordinate (proxy for trade area)",
        },
        "demographics_acs": acs or {"note": "ACS not available; propose data collection plan"},
        "platform_links": inputs.platform_links,
        "competitors_google": comp_brief,
        "competitors_user_input": inputs.competitors,
        "competitor_menu_snapshot": inputs.competitor_menu_snapshot,
        "your_menu_snapshot": inputs.your_menu_snapshot,
        "order_upload_meta": inputs.order_upload_meta,
        "extra_business_context": inputs.extra_business_context,
        "report_requirements": {
            "min_pages_target": min_pages_target,
            "must_include_price_reco_tables": True,
            "must_include_bundle_design": True,
            "must_include_virtual_brand_plan": True
        }
    }

    return f"""
ä½ æ˜¯ AuraInsight çš„å’¨è¯¢é¡¾é—®ã€‚è¯·åŸºäºè¾“å…¥ JSONï¼Œè¾“å‡ºä¸€ä»½â€œéº¦è‚¯é”¡é£æ ¼â€çš„ã€Šé—¨åº—å•†åœˆä¸å¢é•¿è¯Šæ–­æŠ¥å‘Šã€‹æ–‡æœ¬ï¼Œä¸­æ–‡ä¸ºä¸»ï¼Œå…è®¸å°‘é‡è‹±æ–‡æ ‡é¢˜ã€‚
å¿…é¡»éµå®ˆï¼š

A) ä¸¥ç¦è¾“å‡º Markdown è¯­æ³•ï¼ˆä¸è¦å‡ºç°ï¼š#ã€##ã€**ã€|---|ã€```ã€[]()ï¼‰ã€‚å¦åˆ™ç®—å¤±è´¥ã€‚
B) æŠ¥å‘Šåªèƒ½ç”¨ä»¥ä¸‹ç»“æ„æ ‡è®°ï¼š
   - ç« èŠ‚æ ‡é¢˜ç”¨ï¼š ã€ç« èŠ‚æ ‡é¢˜ã€‘
   - åˆ—è¡¨ç”¨ï¼š - æ–‡å­—
   - å°è¡¨æ ¼ç”¨ï¼š è¡¨æ ¼: ç„¶åç”¨â€œåˆ—1,åˆ—2,åˆ—3â€CSVæ ·å¼è¾“å‡ºï¼ˆæœ€å¤š12è¡Œ/è¡¨ï¼‰
C) æ¯ç« å¼€å¤´å¿…é¡»å…ˆç»™ 3â€“6 æ¡ Key Takeawaysï¼ˆçŸ­å¥ã€å¯éªŒè¯ã€å«æ•°å­—ä¼˜å…ˆï¼‰ã€‚
D) æ‰€æœ‰å»ºè®®å¿…é¡»â€œå¯éªŒè¯â€ï¼šæ¯æ¡å»ºè®®åŒ…å«ã€åŠ¨ä½œã€‘ã€åŸå› ã€‘ã€é¢„æœŸå½±å“ã€‘ã€KPIã€‘ã€2å‘¨éªŒè¯æ–¹æ³•ã€‘ã€‚
E) å¿…é¡»åº”ç”¨ï¼šSTPã€JTBDã€Menu Engineeringï¼ˆæ˜Ÿ/ç‰›/è°œ/ç‹—ï¼‰ã€Anchoringï¼ˆé”šç‚¹å®šä»·ï¼‰ã€ERRCï¼ˆè“æµ·å››åŠ¨ä½œï¼‰ï¼Œå¹¶è§£é‡Šä¸ºä½•é€‚ç”¨äºè¯¥å•†åœˆä¸ç«å¯¹ã€‚
F) ä¸èƒ½ç¼–é€ â€œå…·ä½“ä»·æ ¼/å…·ä½“ç«å¯¹ä»·æ ¼â€ã€‚å¦‚æœç¼ºå°‘â€œå½“å‰ä»·/ç«å¯¹ä»·â€ï¼Œå¿…é¡»å†™â€œå¾…è¡¥é½â€ï¼Œå¹¶ç»™å‡ºè¡¥é½æ–¹æ³•ã€‚ä½†åªè¦è¾“å…¥é‡Œå‡ºç°äº†èœå•å¿«ç…§ï¼Œå°±è¦ç»™åˆ°å…·ä½“æ”¹ä»·å»ºè®®ã€‚
G) æŠ¥å‘Šå†…å®¹å¿…é¡»è¶³å¤Ÿé•¿ï¼šä»¥â€œæœ€ç»ˆæ¸²æŸ“ PDF é¢„è®¡ä¸å°‘äº {min_pages_target} é¡µâ€ä¸ºç›®æ ‡ã€‚è¯·ä¸»åŠ¨æ‰©å†™ï¼šç»™å‡ºæ›´å¤šå¯æ‰§è¡Œæ­¥éª¤ã€è¡¨æ ¼ã€å¥—é¤è®¾è®¡ã€å®šä»·åˆ†å±‚ã€ä¿ƒé”€æ—¥å†ã€å¹³å°è¿è¥ SOPã€30/60/90 è®¡åˆ’çš„ä»»åŠ¡æ¸…å•ã€‚

æŠ¥å‘Šä¿¡æ¯ï¼š
- æŠ¥å‘Šæ—¥æœŸï¼š{inputs.report_date}
- å•†å®¶ä¸­æ–‡åï¼š{inputs.restaurant_cn}
- å•†å®¶è‹±æ–‡åï¼š{inputs.restaurant_en}
- åœ°å€ï¼š{inputs.address}
- é…é€åŠå¾„ï¼š{inputs.radius_miles} miles

è¾“å‡ºç« èŠ‚é¡ºåºå¿…é¡»å¦‚ä¸‹ï¼ˆæ ‡é¢˜ä¸€å­—ä¸å·®ï¼‰ï¼š
ã€Executive Summaryã€‘
ã€1. Trade Area & Demographicsã€‘
ã€2. Customer Segments & JTBDã€‘
ã€3. Platform Ecosystem Strategyã€‘
ã€4. Competitive Landscape & Differentiationã€‘
ã€5. Pricing, Anchors & Promo Economicsã€‘
ã€6. Menu Architecture & Menu Engineeringã€‘
ã€7. Operating Playbook & 30/60/90 Roadmapã€‘
ã€Data Gaps & How to Collectã€‘

åœ¨ç¬¬5ã€6ã€7ç« å¿…é¡»åŒ…å«ï¼š
- è¡¨æ ¼: â€œæ ¸å¿ƒSKU,å½“å‰ä»·,å»ºè®®ä»·,é€‚ç”¨å¹³å°,ç†ç”±(ç«å¯¹/ä»·å€¼æ„Ÿ/æˆæœ¬/é”šç‚¹),2å‘¨éªŒè¯KPIâ€
- è¡¨æ ¼: â€œå¥—é¤åç§°,åŒ…å«SKU,æ ‡ä»·,æŠ˜åä»·,é”šç‚¹é€»è¾‘,ç›®æ ‡å®¢ç¾¤,æ¯›åˆ©/é£æ§è¦ç‚¹â€
- è‡³å°‘ 6 å¥—å¥—é¤ï¼ˆå•äººã€åŒäººã€å®¶åº­ã€ä¸‹åˆèŒ¶ã€å®µå¤œã€å¼•æµçˆ†æ¬¾ï¼‰
- è™šæ‹Ÿå“ç‰Œæ–¹æ¡ˆï¼ˆä¾‹å¦‚â€œåè®°å†°å®¤â€ï¼‰ï¼šå®šä½ã€ä¸»æ‰“SKUã€ä»·æ ¼å¸¦ã€ä¸Šæ–°èŠ‚å¥ã€å¹³å°åˆ†å·¥ã€KPIã€é¿å…èš•é£Ÿä¸»åº—è§„åˆ™

è¾“å…¥ JSONï¼š
{json.dumps(data_blob, ensure_ascii=False, indent=2)}
""".strip()


def build_expand_prompt(existing_report: str, min_pages_target: int = 6) -> str:
    return f"""
ä½ æ˜¯ä¸€åé¡¶çº§é¤é¥®å¢é•¿å’¨è¯¢é¡¾é—®ã€‚ä¸‹é¢æ˜¯ä¸€ä»½å·²ç”Ÿæˆçš„æŠ¥å‘Šï¼Œä½†å®ƒè¿˜ä¸å¤Ÿé•¿ã€ä¸å¤Ÿå¯æ‰§è¡Œã€‚
è¯·åœ¨ä¸æ”¹å˜ç« èŠ‚æ ‡é¢˜é¡ºåºä¸æ ‡é¢˜åç§°çš„å‰æä¸‹ï¼ŒæŠŠæ¯ç« å†…å®¹æ‰©å†™å¾—æ›´æ·±ã€æ›´è½åœ°ï¼Œä»¥â€œæœ€ç»ˆæ¸²æŸ“ PDF é¢„è®¡ä¸å°‘äº {min_pages_target} é¡µâ€ä¸ºç›®æ ‡ã€‚

ç¡¬æ€§è¦æ±‚ï¼š
1) ä»ç„¶ä¸¥ç¦è¾“å‡º Markdownï¼ˆä¸è¦å‡ºç° #ã€##ã€**ã€```ã€|---|ã€[]()ï¼‰ã€‚
2) ä»ç„¶å¿…é¡»ä½¿ç”¨ã€ç« èŠ‚æ ‡é¢˜ã€‘ã€- åˆ—è¡¨ã€è¡¨æ ¼: CSVã€‚
3) å¿…é¡»æ–°å¢æ›´å¤šâ€œå¯æ‰§è¡Œç»†èŠ‚â€ï¼š
   - æ›´ç»†çš„æ­¥éª¤ï¼ˆåˆ°æ¯æ—¥/æ¯å‘¨ï¼‰
   - æ›´å¤šå®šä»·/å¥—é¤è¡¨æ ¼ï¼ˆè‡³å°‘å†åŠ  2 å¼ è¡¨ï¼‰
   - æ›´ç»†çš„å¹³å°è¿è¥SOPï¼ˆä¸Šæ¶ã€é¦–å›¾ã€æè¿°ã€åŠ è´­ã€è¯„è®ºã€ä¿ƒé”€ã€ä½£é‡‘/é…é€ç­–ç•¥ï¼‰
   - æ›´å¼ºçš„å¯¹æ ‡é€»è¾‘ï¼ˆç«å¯¹å·®å¼‚åŒ–ç‚¹ã€æˆ‘ä»¬è¦æ‰“çš„â€œè“æµ·â€ï¼‰
4) æ¯æ¡å»ºè®®éƒ½è¦åŒ…å«ï¼šåŠ¨ä½œ/åŸå› /é¢„æœŸå½±å“/KPI/2å‘¨éªŒè¯æ–¹æ³•ã€‚

è¯·å¯¹ä¸‹é¢æ–‡æœ¬è¿›è¡Œâ€œå¢å¼ºæ”¹å†™â€ï¼Œè¾“å‡ºå®Œæ•´æŠ¥å‘Šå…¨æ–‡ï¼ˆä»æŒ‰ç« èŠ‚ç»“æ„è¾“å‡ºï¼‰ï¼š
{existing_report}
""".strip()


# =========================================================
# PDF Rendering (fix cover duplication + layout squeeze)
# =========================================================
def draw_bg(c: canvas.Canvas, bg_path: str):
    if bg_path and os.path.exists(bg_path):
        c.drawImage(bg_path, 0, 0, width=PAGE_W, height=PAGE_H, mask="auto")

def wrap_lines(text: str, max_chars: int) -> List[str]:
    lines: List[str] = []
    for para in text.splitlines():
        para = para.rstrip()
        if not para.strip():
            lines.append("")
            continue
        lines.extend(textwrap.wrap(
            para, width=max_chars,
            break_long_words=False, replace_whitespace=False
        ))
    return lines

def draw_footer(c: canvas.Canvas, report_date: str, page_num: int):
    c.setFillColor(colors.HexColor("#7A7A7A"))
    c.setFont(f_en(False), 8)
    c.drawString(0.75 * inch, 0.55 * inch, f"Confidential | Generated by AuraInsight | {report_date}")
    c.drawRightString(PAGE_W - 0.75 * inch, 0.55 * inch, f"Page {page_num}")
    c.setFillColor(colors.black)

def parse_sections(text: str) -> List[Tuple[str, str]]:
    text = text.strip()
    pattern = r'(ã€[^ã€ã€‘]+ã€‘)'
    parts = re.split(pattern, text)
    sections = []
    cur_title = None
    cur_body = []
    for p in parts:
        if not p:
            continue
        if p.startswith("ã€") and p.endswith("ã€‘"):
            if cur_title is not None:
                sections.append((cur_title.replace("ã€", "").replace("ã€‘", "").strip(), "\n".join(cur_body).strip()))
            cur_title = p
            cur_body = []
        else:
            cur_body.append(p)
    if cur_title is not None:
        sections.append((cur_title.replace("ã€", "").replace("ã€‘", "").strip(), "\n".join(cur_body).strip()))
    return sections

def estimate_pages(report_text: str, max_chars: int = 105, lines_per_page: int = 44) -> int:
    """
    Rough estimator based on wrapped lines.
    """
    lines = wrap_lines(report_text, max_chars=max_chars)
    # add a bit for headings spacing
    n = max(1, math.ceil(len(lines) / lines_per_page))
    return int(n)

def render_pdf(report_text: str, inputs: ReportInputs) -> str:
    register_aurainsight_fonts()
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    safe_name = "".join([ch if ch.isalnum() or ch in ("_", "-", " ") else "_" for ch in inputs.restaurant_en]).strip()
    safe_name = safe_name.replace(" ", "_") or "Restaurant"
    filename = f"AuraInsight_{safe_name}_{inputs.report_date.replace('/','-')}.pdf"
    out_path = os.path.join(OUTPUT_DIR, filename)

    c = canvas.Canvas(out_path, pagesize=letter)

    # ---- Cover ----
    draw_bg(c, BG_COVER)

    # Only draw dynamic fields to avoid duplicated branding
    # (Your cover background already contains AuraInsight + report title)
    # Date
    c.setFillColor(colors.HexColor("#333333"))
    c.setFont(f_en(False), 11)
    c.drawCentredString(PAGE_W / 2, 260, inputs.report_date)

    # Restaurant names
    c.setFillColor(colors.black)
    c.setFont(f_cn(True), 15)
    c.drawCentredString(PAGE_W / 2, 165, inputs.restaurant_cn or inputs.restaurant_en)

    c.setFillColor(colors.HexColor("#333333"))
    c.setFont(f_en(False), 12)
    c.drawCentredString(PAGE_W / 2, 144, inputs.restaurant_en)

    # Address
    c.setFont(f_en(False), 10)
    c.drawCentredString(PAGE_W / 2, 124, inputs.address)

    c.showPage()

    # ---- Content pages ----
    draw_bg(c, BG_CONTENT)
    page_num = 1

    left = 0.85 * inch

    # IMPORTANT: start lower to avoid the blue header bars area
    # Your screenshot shows overlap; so we push content down.
    if CONTENT_BG_HAS_HEADER_BARS:
        top = PAGE_H - 2.05 * inch
    else:
        top = PAGE_H - 1.05 * inch

    y = top

    def new_page():
        nonlocal y, page_num
        draw_footer(c, inputs.report_date, page_num)
        c.showPage()
        page_num += 1
        draw_bg(c, BG_CONTENT)
        y = top

    def draw_heading(title: str):
        nonlocal y
        if y < 1.8 * inch:
            new_page()
        c.setFillColor(colors.black)
        font = f_cn(True) if any("\u4e00" <= ch <= "\u9fff" for ch in title) else f_en(True)
        c.setFont(font, 13)
        c.drawString(left, y, title[:120])
        y -= 18

    def draw_body(text: str):
        nonlocal y
        # More breathable line height + earlier page break
        max_chars = 105
        for line in wrap_lines(text, max_chars):
            if y < 1.25 * inch:
                new_page()
            font = f_en(False) if is_ascii_line(line) else f_cn(False)
            c.setFillColor(colors.black)
            c.setFont(font, 10.2)
            c.drawString(left, y, line)
            y -= 14.5
        y -= 10

    sections = parse_sections(report_text)
    if not sections:
        draw_body(report_text)
    else:
        for title, body in sections:
            draw_heading(title)
            if body:
                draw_body(body)

    draw_footer(c, inputs.report_date, page_num)
    c.save()
    return out_path


# =========================================================
# Order Upload (Lightweight meta extraction)
# =========================================================
def summarize_uploaded_orders(files: List[Any]) -> Dict[str, Any]:
    meta = {"files": [], "notes": "Provide platform exports (CSV). System summarizes schema for analysis."}
    for f in files:
        try:
            df = pd.read_csv(f)
            cols = list(df.columns)[:60]
            meta["files"].append({
                "name": getattr(f, "name", "uploaded.csv"),
                "rows": int(df.shape[0]),
                "cols_sample": cols,
                "date_col_guess": next((c for c in cols if "date" in c.lower() or "time" in c.lower()), None),
            })
        except Exception as e:
            meta["files"].append({
                "name": getattr(f, "name", "uploaded"),
                "error": str(e)[:200]
            })
    return meta


# =========================================================
# UI
# =========================================================
st.title(APP_TITLE)

google_key = st.secrets.get("GOOGLE_MAPS_API_KEY", "")
openai_key = st.secrets.get("OPENAI_API_KEY", "")

with st.sidebar:
    st.header("é…ç½®")
    model = st.selectbox("OpenAI æ¨¡å‹", ["gpt-4o-mini", "gpt-4.1-mini", "gpt-4o"], index=0)
    radius_miles = st.slider("å•†åœˆåŠå¾„ï¼ˆmilesï¼‰", 1.0, 6.0, 4.0, 0.5)
    nearby_radius_m = st.slider("Google Nearby æœç´¢åŠå¾„ï¼ˆç±³ï¼‰", 300, 3000, 1200, 100)
    st.divider()
    st.caption("å­—ä½“éœ€å­˜åœ¨äº assets/fonts/ï¼š")
    st.code(
        "NotoSansSC-Regular.ttf\n"
        "NotoSansSC-Bold.ttf\n"
        "Roboto-Regular.ttf\n"
        "Roboto-Bold.ttf\n"
        "Roboto-Italic.ttf"
    )
    st.divider()
    logout_button()
    st.markdown("---")
    st.caption("Built by c8geek")
    st.markdown("[LinkedIn](https://www.linkedin.com/)")  # ä½ è¦æ›¿æ¢æˆä½ è‡ªå·±çš„ LinkedIn URL


if not google_key:
    st.warning("æœªæ£€æµ‹åˆ° GOOGLE_MAPS_API_KEYï¼Œè¯·åœ¨ .streamlit/secrets.toml é…ç½®ã€‚")
if not openai_key:
    st.warning("æœªæ£€æµ‹åˆ° OPENAI_API_KEYï¼Œè¯·åœ¨ .streamlit/secrets.toml é…ç½®ã€‚")


# =========================================================
# Step 1: Address -> Nearby -> Select restaurant
# =========================================================
st.subheader("Step 1ï½œè¾“å…¥åœ°å€ â†’ æœç´¢é™„è¿‘é¤å…")
address_input = st.text_input("è¾“å…¥åœ°å€ï¼ˆç”¨äºå®šä½å¹¶æœç´¢é™„è¿‘é¤å…ï¼‰", value="2406 19th Ave, San Francisco, CA 94116")

colA, colB = st.columns([1, 1])
with colA:
    if st.button("æœç´¢é™„è¿‘é¤å…", type="primary", disabled=not google_key):
        geo = google_geocode(address_input, google_key)
        if not geo:
            st.error("æ— æ³•è§£æåœ°å€ï¼Œè¯·è¾“å…¥æ›´å®Œæ•´åœ°å€ï¼ˆå«åŸå¸‚/å·ï¼‰ã€‚")
        else:
            lat, lng = geo
            places = google_nearby_restaurants(lat, lng, google_key, radius_m=nearby_radius_m)
            st.session_state["geo"] = (lat, lng)
            st.session_state["places"] = places
            st.success(f"å·²æ‰¾åˆ° {len(places)} å®¶é™„è¿‘é¤å…ã€‚")

places = st.session_state.get("places", [])
selected_place_id = None

if places:
    options, id_map = [], {}
    for p in places:
        name = p.get("name", "")
        addr = p.get("vicinity", "")
        rating = p.get("rating", "NA")
        total = p.get("user_ratings_total", "NA")
        pid = p.get("place_id", "")
        label = f"{name} | {addr} | â­{rating} ({total})"
        options.append(label)
        id_map[label] = pid

    selected_label = st.selectbox("é€‰æ‹©ç›®æ ‡é¤å…ï¼ˆGoogle Nearbyï¼‰", options)
    selected_place_id = id_map.get(selected_label)

    if st.button("æ‹‰å–é¤å…è¯¦æƒ…ï¼ˆGoogle Place Detailsï¼‰", disabled=not google_key):
        if not selected_place_id:
            st.error("è¯·å…ˆé€‰æ‹©ä¸€ä¸ªé¤å…ã€‚")
        else:
            details = google_place_details(selected_place_id, google_key)
            if not details:
                st.error("æ‹‰å–è¯¦æƒ…å¤±è´¥ã€‚")
            else:
                st.session_state["place_details"] = details
                st.success("å·²æ‹‰å–é¤å…è¯¦æƒ…ã€‚")

place_details = st.session_state.get("place_details", {})


# =========================================================
# Step 2: Demographics + platform links + uploads
# =========================================================
if place_details:
    st.subheader("Step 2ï½œè¡¥é½å•†åœˆç”»åƒ + ç«å¯¹/è‡ªå®¶èœå•ä¸Šä¼  + æ•°æ®è¡¥å½•å…¥å£")

    rest_lat = None
    rest_lng = None
    try:
        loc = place_details.get("geometry", {}).get("location", {})
        rest_lat = float(loc.get("lat"))
        rest_lng = float(loc.get("lng"))
    except Exception:
        pass

    col1, col2 = st.columns([1, 1])

    with col1:
        restaurant_en = st.text_input("é¤å…è‹±æ–‡å", value=place_details.get("name", ""))
        restaurant_cn = st.text_input("é¤å…ä¸­æ–‡åï¼ˆå¯é€‰ï¼‰", value="")
        formatted_address = st.text_input("é¤å…åœ°å€", value=place_details.get("formatted_address", address_input))

        rating = place_details.get("rating", "")
        total = place_details.get("user_ratings_total", "")
        st.caption(f"Google æ•°æ®ï¼šâ­{rating}ï¼ˆ{total} reviewsï¼‰")

        extra_context = st.text_area(
            "è¡¥å……ä¸šåŠ¡èƒŒæ™¯ï¼ˆå¯é€‰ï¼‰",
            value="ä¾‹å¦‚ï¼šç»è¥å¹´é™ã€ä¸»æ‰“èœã€ç›®æ ‡å®¢ç¾¤ã€å½“å‰ç—›ç‚¹ï¼ˆå•é‡/è¯„åˆ†/åˆ©æ¶¦/äººæ‰‹ç­‰ï¼‰ã€‚",
            height=120
        )

    with col2:
        st.markdown("### å¹³å°é“¾æ¥ï¼ˆé—¨åº—è‡ªèº«ï¼‰")
        direct_url = st.text_input("Direct / order.online", value="")
        uber_url = st.text_input("Uber Eats", value="")
        doordash_url = st.text_input("DoorDashï¼ˆå¯é€‰ï¼‰", value="")
        fantuan_url = st.text_input("é¥­å›¢ Fantuan", value="")
        panda_url = st.text_input("HungryPanda ç†ŠçŒ«", value="")

    # ACS
    with st.expander("è‡ªåŠ¨è·å–å•†åœˆäººå£/æ”¶å…¥/å¹´é¾„/æ—è£”/ç§Ÿä½æ¯”ä¾‹ï¼ˆUS Census ACSï¼‰", expanded=True):
        if rest_lat and rest_lng:
            if st.button("è·å– ACS å•†åœˆç”»åƒï¼ˆè‡ªåŠ¨ï¼‰"):
                tract_info = census_tract_from_latlng(rest_lat, rest_lng)
                if not tract_info:
                    st.warning("æ— æ³•è·å– tract ä¿¡æ¯ï¼ˆCensus geocoderï¼‰ã€‚")
                else:
                    acs_data = acs_5y_profile(
                        state=tract_info["STATE"],
                        county=tract_info["COUNTY"],
                        tract=tract_info["TRACT"],
                        year=2023
                    )
                    st.session_state["tract_info"] = tract_info
                    st.session_state["acs_data"] = acs_data
                    if acs_data:
                        st.success("å·²è·å– ACS æ•°æ®ï¼ˆtract çº§åˆ«ä»£ç†ï¼‰ã€‚")
                    else:
                        st.warning("ACS æ•°æ®è¿”å›ä¸ºç©ºã€‚")
        else:
            st.info("æœªèƒ½ä» Google Place Details è·å–åæ ‡ï¼Œæ— æ³•è°ƒç”¨ ACSã€‚")

        acs_data = st.session_state.get("acs_data", None)
        if acs_data:
            pop = acs_data.get("pop_total")
            inc = acs_data.get("median_income")
            age = acs_data.get("median_age")
            pct_asian = acs_data.get("pct_asian")
            pct_renter = acs_data.get("pct_renter")
            st.write({
                "ACS Year": acs_data.get("year"),
                "Geography": acs_data.get("name"),
                "Population (tract)": None if pop is None else int(pop),
                "Median HH Income": None if inc is None else f"${int(inc):,}",
                "Median Age": age,
                "% Asian (proxy)": None if pct_asian is None else f"{pct_asian*100:.1f}%",
                "% Renter (proxy)": None if pct_renter is None else f"{pct_renter*100:.1f}%",
                "Note": "ACS ä¸º tract çº§åˆ«ä»£ç†ï¼Œä½œä¸º 3â€“4 miles å•†åœˆè¿‘ä¼¼ç”»åƒï¼›æŠ¥å‘Šä¸­ä¼šæ˜ç¡®è¯¥å‡è®¾ã€‚"
            })

    # Competitors editor
    st.markdown("### ç«å¯¹ä¿¡æ¯ï¼ˆå¯å¢åˆ è¡Œï¼šç”¨äºå·®å¼‚åŒ–ä¸èœå•ç­–ç•¥ï¼‰")
    default_comp = pd.DataFrame([
        {"name": "Smile House Cafe", "ubereats": "", "doordash": "", "fantuan": "", "hungrypanda": "", "website": "", "notes": ""},
        {"name": "å‡¤å‡°èšä¼š", "ubereats": "", "doordash": "", "fantuan": "", "hungrypanda": "", "website": "", "notes": ""},
        {"name": "å¤§å®¶ä¹", "ubereats": "", "doordash": "", "fantuan": "", "hungrypanda": "", "website": "", "notes": ""},
    ])
    comp_df = st.data_editor(
        default_comp,
        num_rows="dynamic",
        use_container_width=True,
        key="comp_editor"
    )
    competitors = comp_df.fillna("").to_dict("records")

    # Optional: pull competitor Google details
    competitor_places = st.session_state.get("competitor_places", [])
    colx, coly = st.columns([1, 1])
    with colx:
        if st.button("ï¼ˆå¯é€‰ï¼‰æ‹‰å–ç«å¯¹ Google æ•°æ®", disabled=not google_key):
            pulled = []
            for row in competitors[:10]:
                nm = (row.get("name") or "").strip()
                if not nm:
                    continue
                pid = google_textsearch_place_id(f"{nm} San Francisco", google_key)
                if pid:
                    pulled.append(google_place_details(pid, google_key))
            st.session_state["competitor_places"] = pulled
            competitor_places = pulled
            st.success(f"å·²æ‹‰å– {len(pulled)} å®¶ç«å¯¹ Google è¯¦æƒ…ã€‚")
    with coly:
        st.caption("æç¤ºï¼šGoogle æ•°æ®ç”¨äºä¿¡ä»»èµ„äº§å¯¹æ¯”ï¼›èœå•/ä»·æ ¼å»ºè®®ä¸»è¦ä¾èµ–ä½ ä¸Šä¼ çš„â€œç«å¯¹/è‡ªå®¶èœå•å¿«ç…§â€ã€‚")

    # ===== NEW: Upload competitor menu snapshot files =====
    st.markdown("### èœå•å¿«ç…§ä¸Šä¼ ï¼ˆç”¨äºç»™å‡ºå…·ä½“èœå“/ä»·æ ¼/å¥—é¤å»ºè®®ï¼‰")
    colm1, colm2 = st.columns([1, 1])

    with colm1:
        comp_menu_files = st.file_uploader(
            "ä¸Šä¼  Competitor menu snapshotï¼ˆtxt/csv/xlsxï¼Œå¯å¤šé€‰ï¼‰",
            type=["txt", "csv", "xlsx", "xls"],
            accept_multiple_files=True
        )
        comp_menu_snapshot = build_menu_snapshot_from_uploads(comp_menu_files) if comp_menu_files else ""
        if comp_menu_snapshot:
            st.success("å·²è¯»å–ç«å¯¹èœå•å¿«ç…§ã€‚")
            with st.expander("é¢„è§ˆç«å¯¹èœå•å¿«ç…§ï¼ˆè‡ªåŠ¨æ•´ç†æ–‡æœ¬ï¼‰", expanded=False):
                st.text(comp_menu_snapshot[:4000])

    with colm2:
        your_menu_files = st.file_uploader(
            "ä¸Šä¼  Your menu snapshotï¼ˆtxt/csv/xlsxï¼Œå¯å¤šé€‰ï¼‰",
            type=["txt", "csv", "xlsx", "xls"],
            accept_multiple_files=True
        )
        your_menu_snapshot = build_menu_snapshot_from_uploads(your_menu_files) if your_menu_files else ""
        if your_menu_snapshot:
            st.success("å·²è¯»å–è‡ªå®¶èœå•å¿«ç…§ã€‚")
            with st.expander("é¢„è§ˆè‡ªå®¶èœå•å¿«ç…§ï¼ˆè‡ªåŠ¨æ•´ç†æ–‡æœ¬ï¼‰", expanded=False):
                st.text(your_menu_snapshot[:4000])

    # Orders upload
    with st.expander("ä¸Šä¼ è®¢å•æŠ¥è¡¨ï¼ˆCSVï¼Œå¯é€‰ï¼šç”¨äºæ—¶æ®µ/å®¢å•/çƒ­é”€/KPIï¼‰", expanded=False):
        uploads = st.file_uploader("ä¸Šä¼ å¹³å°è®¢å•å¯¼å‡º CSVï¼ˆå¯å¤šé€‰ï¼‰", type=["csv"], accept_multiple_files=True)
        order_meta = {}
        if uploads:
            order_meta = summarize_uploaded_orders(uploads)
            st.json(order_meta)
        else:
            order_meta = {"files": [], "note": "No uploads"}

    # =========================================================
    # Step 3: Generate report (ensure >=6 pages)
    # =========================================================
    st.subheader("Step 3ï½œç”Ÿæˆæ·±åº¦åˆ†ææŠ¥å‘Šï¼ˆå’¨è¯¢çº§ï¼Œç›®æ ‡â‰¥6é¡µï¼‰")
    report_date = dt.datetime.now().strftime("%m/%d/%Y")

    platform_links = {
        "direct": direct_url.strip(),
        "uber_eats": uber_url.strip(),
        "doordash": doordash_url.strip(),
        "fantuan": fantuan_url.strip(),
        "hungrypanda": panda_url.strip(),
    }

    min_pages_target = 6

    if st.button("ç”ŸæˆæŠ¥å‘Šå†…å®¹", type="primary", disabled=not openai_key):
        inputs = ReportInputs(
            report_date=report_date,
            restaurant_cn=(restaurant_cn.strip() or restaurant_en.strip()),
            restaurant_en=restaurant_en.strip(),
            address=formatted_address.strip(),
            radius_miles=radius_miles,
            platform_links=platform_links,
            competitors=competitors,
            competitor_menu_snapshot=comp_menu_snapshot.strip(),
            your_menu_snapshot=your_menu_snapshot.strip(),
            order_upload_meta=order_meta,
            extra_business_context=extra_context.strip(),
        )

        prompt = build_prompt(
            place=place_details,
            inputs=inputs,
            competitor_places=competitor_places,
            acs=st.session_state.get("acs_data", None),
            min_pages_target=min_pages_target,
        )

        with st.spinner("æ­£åœ¨ç”Ÿæˆå’¨è¯¢çº§æŠ¥å‘Šï¼ˆæ·±åº¦å•†åœˆç”»åƒ + èœå•/å®šä»·/è“æµ·ç­–ç•¥ï¼‰..."):
            report_text = openai_generate(prompt, openai_key, model=model)
            report_text = sanitize_text(report_text)

        # Auto expand until estimated pages >= min_pages_target (max 3 passes)
        for _ in range(2):
            pages_est = estimate_pages(report_text)
            if pages_est >= min_pages_target:
                break
            with st.spinner(f"æŠ¥å‘ŠåçŸ­ï¼ˆé¢„è®¡ {pages_est} é¡µï¼‰ï¼Œæ­£åœ¨è‡ªåŠ¨æ‰©å†™è‡³ â‰¥{min_pages_target} é¡µ..."):
                exp_prompt = build_expand_prompt(report_text, min_pages_target=min_pages_target)
                report_text = openai_generate(exp_prompt, openai_key, model=model)
                report_text = sanitize_text(report_text)

        st.session_state["report_text"] = report_text
        st.session_state["report_inputs"] = inputs
        st.success(f"æŠ¥å‘Šå†…å®¹å·²ç”Ÿæˆï¼ˆé¢„è®¡é¡µæ•°ï¼š{estimate_pages(report_text)}ï¼‰ã€‚")


# =========================================================
# Preview + PDF
# =========================================================
report_text = st.session_state.get("report_text", "")
report_inputs: Optional[ReportInputs] = st.session_state.get("report_inputs", None)

if report_text and report_inputs:
    st.subheader("é¢„è§ˆï¼ˆå¯ç¼–è¾‘ï¼‰")
    edited = st.text_area("æŠ¥å‘Šæ­£æ–‡ï¼ˆä½ å¯ä»¥ç›´æ¥ä¿®æ”¹ï¼‰", value=report_text, height=520)
    st.session_state["report_text"] = sanitize_text(edited)

    st.subheader("Step 4ï½œç”Ÿæˆ PDFï¼ˆå¥—ç”¨å°é¢/å†…å®¹é¡µèƒŒæ™¯å›¾ï¼‰")

    warn = []
    if not os.path.exists(BG_COVER):
        warn.append(f"å°é¢èƒŒæ™¯å›¾ä¸å­˜åœ¨ï¼š{BG_COVER}")
    if not os.path.exists(BG_CONTENT):
        warn.append(f"å†…å®¹é¡µèƒŒæ™¯å›¾ä¸å­˜åœ¨ï¼š{BG_CONTENT}")
    if warn:
        st.warning("\n".join(warn))

    if st.button("ç”Ÿæˆ PDF", type="primary"):
        with st.spinner("æ­£åœ¨ç”Ÿæˆ PDF..."):
            pdf_path = render_pdf(
                report_text=st.session_state["report_text"],
                inputs=report_inputs,
            )
        st.success("PDF ç”Ÿæˆå®Œæˆã€‚")
        with open(pdf_path, "rb") as f:
            st.download_button(
                "ä¸‹è½½ PDF",
                f,
                file_name=os.path.basename(pdf_path),
                mime="application/pdf"
            )
        st.caption(f"è¾“å‡ºè·¯å¾„ï¼š{pdf_path}")
else:
    st.info("å®Œæˆé¤å…é€‰æ‹©å¹¶ç”ŸæˆæŠ¥å‘Šåï¼Œè¿™é‡Œä¼šæ˜¾ç¤ºé¢„è§ˆä¸ PDF ä¸‹è½½ã€‚")
